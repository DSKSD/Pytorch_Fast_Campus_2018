{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from konlpy.tag import Kkma\n",
    "import torch.utils.data as torchdata\n",
    "from torchtext.data import Field\n",
    "tagger = Kkma()\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.1\n",
      "3.2.4\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if USE_CUDA else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<unk>\"], seq))\n",
    "    return LongTensor(idxs)\n",
    "\n",
    "def prepare_word(word, word2index):\n",
    "    return LongTensor([word2index[word]]) if word2index.get(word) is not None else LongTensor([word2index[\"<unk>\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 로드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = open('data/corpus.txt','r',encoding=\"utf-8\").readlines()\n",
    "corpus = [c[:-1] for c in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['음 저는 구글이 저 시스템을 지원할거라 믿습니다. 이번 eager처럼 파이토치에서 좋아보이는건 유저 안뺏기려 사람 갈아 넣는중이니..',\n",
       " '시스템 보니까 좋은 건 다 갈아넣었던데 좀 써봐야겠고... 기술된 것만 봐서는 추후 모델 프로토타이핑은 그냥 파이토치로 넘어가야하지 않나 싶을 정도.',\n",
       " '진짜 올해 안에 파이토치로 넘어가야하나? 넘어가도 될 유도가 하나씩 늘어나고 있음',\n",
       " '파이토치 한번도 공부 안 해봤는데 마이그레이션 하느라 하도 많이 봤더니 이제 나도 짤 수 있을 것 같음',\n",
       " '하아.. 파이토치 쓰다가 텐서플로 쓰니 넘나 피곤한 것.. -.-;; 확실히 파이토치가 편하게는 만들었구만. 그치만 프로덕션을 생각한다면 확실히 아직은 텐서플로가 압도적으로 우세한 것 같다.',\n",
       " '텐서플로 1.5에 파이토치모드라 불리는(ㅋㅋ) eager 모드의 프리뷰 버전이 들어간다고합니다. 그래프구성+연산을 나눠서하지 않고 합쳐서 하는건데요. 이게 정식으로 들어가면 연구하시는 분들은 텐서플로를 좀 더 편하게 쓰실 수 있을 것 같네요.',\n",
       " '인공지능(AI) 유통상품 인식 기술로 소매점 결제 무인화 앞당긴다  딥러닝과 특징점 매칭 기술로 농산물과 같이 개체마다 모양의 차이가 큰 자연물이나 비정형 상품에서부터 공산품까지 다양한 상품을 동시에 정확하게 인식',\n",
       " '구글 딥러닝 쌩씨발새끼',\n",
       " '근데 딥러닝 뭐시기 비디오게임도 나오지 않을까  보스가 너의 패턴을 학습한다',\n",
       " '브랜돈 포스팅에 따봉 1개월 이상 없는 분들은 페북 딥러닝 IoT 5G 블록체인 ICO 알고리즘으로 인해 자동 페삭됩니다.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized = [tagger.morphs(c) for c in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['음',\n",
       " '저',\n",
       " '는',\n",
       " '구',\n",
       " '글',\n",
       " '이',\n",
       " '저',\n",
       " '시스템',\n",
       " '을',\n",
       " '지원',\n",
       " '하',\n",
       " 'ㄹ',\n",
       " '거',\n",
       " '이',\n",
       " '라',\n",
       " '믿',\n",
       " '습니다',\n",
       " '.',\n",
       " '이번',\n",
       " 'eager',\n",
       " '처럼',\n",
       " '파이',\n",
       " '토치',\n",
       " '에서',\n",
       " '좋',\n",
       " '아',\n",
       " '보이',\n",
       " '는',\n",
       " '것',\n",
       " '은',\n",
       " '유저',\n",
       " '안',\n",
       " '뺏기',\n",
       " '려',\n",
       " '사람',\n",
       " '갈',\n",
       " '아',\n",
       " '넣',\n",
       " '는',\n",
       " '중',\n",
       " '이',\n",
       " '니',\n",
       " '..']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 빈도수가 적은 단어 제외"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stopwords 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_count = Counter(flatten(tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('!!!!!!', 1),\n",
       " ('대학', 1),\n",
       " ('응용', 1),\n",
       " ('깔', 1),\n",
       " ('대면', 1),\n",
       " ('T', 1),\n",
       " ('툴셋', 1),\n",
       " ('파시', 1),\n",
       " ('도움', 1),\n",
       " ('것들', 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(reversed(word_count.most_common()))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_COUNT = 2 # 최소 2번 이상 등장한 단어만 사용\n",
    "stopwords = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for w, c in word_count.items():\n",
    "    if c < MIN_COUNT:\n",
    "        stopwords.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "604"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 단어셋 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = list(set(flatten(tokenized)) - set(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2index = {'<unk>' : 0}\n",
    "for vo in vocab:\n",
    "    if word2index.get(vo) is None:\n",
    "        word2index[vo] = len(word2index)\n",
    "        \n",
    "index2word = {v:k for k, v in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "345"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 학습 데이터 준비 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 5\n",
    "windows =  flatten([list(nltk.ngrams(['<DUMMY>'] * WINDOW_SIZE + c + ['<DUMMY>'] * WINDOW_SIZE, WINDOW_SIZE * 2 + 1)) for c in tokenized])\n",
    "\n",
    "train_data = []\n",
    "\n",
    "for window in windows:\n",
    "    for i in range(WINDOW_SIZE * 2 + 1):\n",
    "        # stopwords에 속하는 단어는 제외\n",
    "        if window[i] in stopwords or window[WINDOW_SIZE] in stopwords: \n",
    "            continue # min_count\n",
    "        if i == WINDOW_SIZE or window[i] == '<DUMMY>': \n",
    "            continue\n",
    "        train_data.append((window[WINDOW_SIZE], window[i]))\n",
    "\n",
    "X_p = []\n",
    "y_p = []\n",
    "\n",
    "for tr in train_data:\n",
    "    X_p.append(prepare_word(tr[0], word2index))\n",
    "    y_p.append(prepare_word(tr[1], word2index))\n",
    "    \n",
    "train_data = list(zip(X_p, y_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20794"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       "  184\n",
       " [torch.LongTensor of size 1], \n",
       "  43\n",
       " [torch.LongTensor of size 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0] # (center word, context word) pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 페어를 토치 데이터셋 클래스로 래핑 후, 데이터로더 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WordPair(torchdata.Dataset):\n",
    "    def __init__(self,dataset):\n",
    "        self.dataset = dataset\n",
    "        self.length = len(self.dataset)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # 인덱스에 해당하는 데이터셋 리턴\n",
    "        return self.dataset[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        # 데이터셋 수\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = WordPair(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_loader = torchdata.DataLoader(dataset=train_data,\n",
    "                                           batch_size=256, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Unigram Distribution**0.75 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(w)=U(w)^{3/4}/Z$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Z = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_count = Counter(flatten(tokenized)) # unigram distribution\n",
    "num_total_words = sum([c for w, c in word_count.items() if w not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unigram_table = []\n",
    "\n",
    "for vo in vocab:\n",
    "    unigram_table.extend([vo] * int(((word_count[vo]/num_total_words)**0.75)/Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344 3572\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab), len(unigram_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Sampling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(w)에서 K개만큼의 Negative samples을 뽑아주는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def negative_sampling(targets, unigram_table, k):\n",
    "    batch_size = targets.size(0)\n",
    "    neg_samples = []\n",
    "    for i in range(batch_size):\n",
    "        nsample = []\n",
    "        target_index = targets[i].tolist()[0]\n",
    "        while len(nsample) < k: # num of sampling\n",
    "            neg = random.choice(unigram_table)\n",
    "            if word2index[neg] == target_index:\n",
    "                continue\n",
    "            nsample.append(neg)\n",
    "        neg_samples.append(prepare_sequence(nsample, word2index).view(1, -1))\n",
    "    \n",
    "    return torch.cat(neg_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SkipgramNegSampling(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, projection_dim):\n",
    "        super(SkipgramNegSampling, self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, projection_dim) # center embedding\n",
    "        self.embedding_u = nn.Embedding(vocab_size, projection_dim) # out embedding\n",
    "        self.logsigmoid = nn.LogSigmoid()\n",
    "                \n",
    "        # xavier init\n",
    "        self.embedding_v.weight.data = nn.init.xavier_uniform(self.embedding_v.weight.data)\n",
    "        self.embedding_u.weight.data = nn.init.xavier_uniform(self.embedding_u.weight.data)\n",
    "        \n",
    "    def forward(self, center_words, target_words, negative_words):\n",
    "        center_embeds = self.embedding_v(center_words) # B x 1 x D\n",
    "        target_embeds = self.embedding_u(target_words) # B x 1 x D\n",
    "        \n",
    "        neg_embeds = -self.embedding_u(negative_words) # B x K x D\n",
    "        \n",
    "        positive_score = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2) # Bx1\n",
    "        negative_score = torch.sum(neg_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2), 1).view(negs.size(0), -1) # BxK -> Bx1\n",
    "        \n",
    "        # loss function\n",
    "        loss = self.logsigmoid(positive_score) + self.logsigmoid(negative_score)\n",
    "        \n",
    "        return -torch.mean(loss)\n",
    "    \n",
    "    def get_embedding(self, inputs):\n",
    "        embeds_v = self.embedding_v(inputs)\n",
    "        embeds_u = self.embedding_u(inputs)\n",
    "        \n",
    "        return (embeds_v+embeds_u)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 30 \n",
    "BATCH_SIZE = 256\n",
    "EPOCH = 100\n",
    "NEG = 10 # Num of Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "model = SkipgramNegSampling(len(word2index), EMBEDDING_SIZE)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0, mean_loss : 1.32\n",
      "Epoch : 10, mean_loss : 0.89\n",
      "Epoch : 20, mean_loss : 0.79\n",
      "Epoch : 30, mean_loss : 0.71\n",
      "Epoch : 40, mean_loss : 0.65\n",
      "Epoch : 50, mean_loss : 0.61\n",
      "Epoch : 60, mean_loss : 0.58\n",
      "Epoch : 70, mean_loss : 0.56\n",
      "Epoch : 80, mean_loss : 0.54\n",
      "Epoch : 90, mean_loss : 0.53\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    for i,(inputs,targets) in enumerate(train_loader):\n",
    "        \n",
    "        # P(w)로부터 Negative Sample NEG개만큼 뽑기\n",
    "        negs = negative_sampling(targets, unigram_table, NEG)\n",
    "        inputs = Variable(inputs) # B x 1\n",
    "        targets = Variable(targets) # B x 1\n",
    "        negs = Variable(negs) # B x K\n",
    "        \n",
    "        model.zero_grad()\n",
    "\n",
    "        loss = model(inputs, targets, negs)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        losses.append(loss.data.tolist()[0])\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch : %d, mean_loss : %.02f\" % (epoch, np.mean(losses)))\n",
    "        losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cosine Similarity\n",
    "\n",
    "def word_similarity(target,index2word,num=10):\n",
    "    target_V = model.get_embedding(Variable(prepare_word(target, word2index))).view(1,-1)\n",
    "    matrix = (model.embedding_u.weight.data + model.embedding_v.weight.data)/2\n",
    "    cosine_sim = F.cosine_similarity(target_V.data, matrix,dim=1,eps=1e-6)\n",
    "    v,i = cosine_sim.topk(num+1)\n",
    "    \n",
    "    return [[index2word[ii],vv] for ii,vv in zip(i.tolist()[1:],v.tolist()[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['딥', 0.9535467624664307],\n",
       " ['머신', 0.7704424262046814],\n",
       " ['뇌', 0.6196351051330566],\n",
       " ['씨', 0.6168031692504883],\n",
       " ['신경망', 0.5668169856071472],\n",
       " ['수업', 0.5373671650886536],\n",
       " ['게임', 0.49644383788108826],\n",
       " ['데', 0.4827418625354767],\n",
       " ['뭐', 0.4823964834213257],\n",
       " ['나요', 0.47976601123809814]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_similarity(\"러닝\",index2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "import pickle\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 텐서보드 포트 설정\n",
    "port = pickle.load(open(\"port.info\",\"rb\"))[os.getcwd().split(\"/\")[-2]]\n",
    "\n",
    "# 텐서보드 데이터 파일 초기화\n",
    "try:\n",
    "    shutil.rmtree('runs/')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = SummaryWriter(comment='-embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matrix = (model.embedding_u.weight.data + model.embedding_v.weight.data)/2\n",
    "label = [index2word[i] for i in range(len(index2word))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([345, 30])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer.add_embedding(matrix,metadata=label)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method add_embedding in module tensorboardX.writer:\n",
      "\n",
      "add_embedding(mat, metadata=None, label_img=None, global_step=None, tag='default') method of tensorboardX.writer.SummaryWriter instance\n",
      "    Add embedding projector data to summary.\n",
      "    \n",
      "    Args:\n",
      "        mat (torch.Tensor): A matrix which each row is the feature vector of the data point\n",
      "        metadata (list): A list of labels, each element will be convert to string\n",
      "        label_img (torch.Tensor): Images correspond to each data point\n",
      "        global_step (int): Global step value to record\n",
      "        tag (string): Name for the embedding\n",
      "    Shape:\n",
      "        mat: :math:`(N, D)`, where N is number of data and D is feature dimension\n",
      "    \n",
      "        label_img: :math:`(N, C, H, W)`\n",
      "    \n",
      "    Examples::\n",
      "    \n",
      "        import keyword\n",
      "        import torch\n",
      "        meta = []\n",
      "        while len(meta)<100:\n",
      "            meta = meta+keyword.kwlist # get some strings\n",
      "        meta = meta[:100]\n",
      "    \n",
      "        for i, v in enumerate(meta):\n",
      "            meta[i] = v+str(i)\n",
      "    \n",
      "        label_img = torch.rand(100, 3, 10, 32)\n",
      "        for i in range(100):\n",
      "            label_img[i]*=i/100.0\n",
      "    \n",
      "        writer.add_embedding(torch.randn(100, 5), metadata=meta, label_img=label_img)\n",
      "        writer.add_embedding(torch.randn(100, 5), label_img=label_img)\n",
      "        writer.add_embedding(torch.randn(100, 5), metadata=meta)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(writer.add_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard 0.4.0rc3 at http://dsksd-tf:6006 (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir runs --port 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer.add_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = vdatasets.MNIST('../../data/MNIST/', train=False, download=True)\n",
    "images = dataset.test_data[:100].float()\n",
    "label = dataset.test_labels[:100]\n",
    "\n",
    "features = images.view(100, 784)\n",
    "writer.add_embedding(features, metadata=label, label_img=images.unsqueeze(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
