{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import nltk\n",
    "from konlpy.tag import Kkma\n",
    "kor_tagger = Kkma()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', ',', 'my', 'name', 'is', 'sungdong', '.', 'What', \"'s\", 'your', 'name', '?']\n"
     ]
    }
   ],
   "source": [
    "token = nltk.word_tokenize(\"Hi, my name is sungdong. What's your name?\")\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕', '하', '세요', '!', '저', '는', '파이', '토치', '를', '공부', '하', '는', '중', '이', 'ㅂ니다', '.']\n"
     ]
    }
   ],
   "source": [
    "token = kor_tagger.morphs(\"안녕하세요! 저는 파이토치를 공부하는 중입니다.\")\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'하': 1, '.': 13, '를': 8, '중': 10, '안녕': 0, '파이': 6, '세요': 2, '토치': 7, '저': 4, '!': 3, '공부': 9, 'ㅂ니다': 12, '이': 11, '는': 5}\n"
     ]
    }
   ],
   "source": [
    "word2index={} # dictionary for indexing\n",
    "for vo in token:\n",
    "    if word2index.get(vo)==None:\n",
    "        word2index[vo]=len(word2index)\n",
    "print(word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encoding(word,word2index):\n",
    "    tensor = torch.zeros(len(word2index))\n",
    "    index = word2index[word]\n",
    "    tensor[index]=1.\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 1\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 14]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch_vector = one_hot_encoding(\"토치\",word2index)\n",
    "print(torch_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py_vector = one_hot_encoding(\"파이\",word2index)\n",
    "py_vector.dot(torch_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = [[\"배고프다 밥줘\",\"FOOD\"],\n",
    "                    [\"뭐 먹을만한거 없냐\",\"FOOD\"],\n",
    "                    [\"맛집 추천\",\"FOOD\"],\n",
    "                    [\"이 근처 맛있는 음식점 좀\",\"FOOD\"],\n",
    "                    [\"밥줘\",\"FOOD\"],\n",
    "                    [\"뭐 먹지?\",\"FOOD\"],\n",
    "                    [\"삼겹살 먹고싶어\",\"FOOD\"],\n",
    "                    [\"영화 보고싶다\",\"MEDIA\"],\n",
    "                    [\"요즘 볼만한거 있어?\",\"MEDIA\"],\n",
    "                    [\"영화나 예능 추천\",\"MEDIA\"],\n",
    "                    [\"재밌는 드라마 보여줘\",\"MEDIA\"],\n",
    "                    [\"신과 함께 줄거리 좀 알려줘\",\"MEDIA\"],\n",
    "                    [\"고등랩퍼 다시보기 좀\",\"MEDIA\"],\n",
    "                    [\"재밌는 영상 하이라이트만 보여줘\",\"MEDIA\"]]\n",
    "\n",
    "test_data = [[\"쭈꾸미 맛집 좀 찾아줘\",\"FOOD\"],\n",
    "                   [\"매콤한 떡볶이 먹고싶다\",\"FOOD\"],\n",
    "                   [\"강남 씨지비 조조 영화 스케줄표 좀\",\"MEDIA\"],\n",
    "                   [\"효리네 민박 보고싶엉\",\"MEDIA\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X,train_y = list(zip(*train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X = [kor_tagger.morphs(x) for x in train_X] # Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['배고프', '다', '밥', '주', '어'],\n",
       " ['뭐', '먹', '을', '만하', 'ㄴ', '거', '없', '냐'],\n",
       " ['맛', '집', '추천'],\n",
       " ['이', '근처', '맛있', '는', '음식', '점', '좀'],\n",
       " ['밥', '주', '어'],\n",
       " ['뭐', '먹', '지', '?'],\n",
       " ['삼겹살', '먹', '고', '싶', '어'],\n",
       " ['영화', '보', '고', '싶', '다'],\n",
       " ['요즘', '볼만', '하', 'ㄴ', '거', '있', '어', '?'],\n",
       " ['영화', '나', '예능', '추천'],\n",
       " ['재밌', '는', '드라마', '보여주', '어'],\n",
       " ['신', '과', '함께', '줄거리', '좀', '알려주', '어'],\n",
       " ['고등', '랩', '푸', '어', '다시', '보', '기', '좀'],\n",
       " ['재밌', '는', '영상', '하이라이트', '만', '보여주', '어']]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build Vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'만': 52, '줄거리': 43, '밥': 3, '푸': 47, '과': 41, '맛': 14, '뭐': 6, '알려주': 44, '다시': 48, '기': 49, '있': 34, '함께': 42, '요즘': 31, '고등': 45, '음식': 21, '점': 22, '랩': 46, '근처': 18, '?': 25, '영화': 29, '싶': 28, '지': 24, '추천': 16, '영상': 50, '좀': 23, '재밌': 37, '맛있': 19, '하': 33, '<unk>': 0, '냐': 13, '집': 15, '어': 5, '배고프': 1, '만하': 9, '주': 4, '고': 27, '예능': 36, '없': 12, '보여주': 39, '하이라이트': 51, 'ㄴ': 10, '먹': 7, '드라마': 38, '삼겹살': 26, '나': 35, '볼만': 32, '보': 30, '거': 11, '다': 2, '신': 40, '이': 17, '을': 8, '는': 20}\n",
      "{'FOOD': 0, 'MEDIA': 1}\n"
     ]
    }
   ],
   "source": [
    "word2index={'<unk>' : 0}\n",
    "for x in train_X:\n",
    "    for token in x:\n",
    "        if word2index.get(token)==None:\n",
    "            word2index[token]=len(word2index)\n",
    "            \n",
    "class2index = {'FOOD' : 0, 'MEDIA' : 1}\n",
    "print(word2index)\n",
    "print(class2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_BoW(seq,word2index):\n",
    "    tensor = torch.zeros(len(word2index))\n",
    "    for w in seq:\n",
    "        index = word2index.get(w)\n",
    "        if index!=None:\n",
    "            tensor[index]+=1.\n",
    "        else:\n",
    "            index = word2index['<unk>']\n",
    "            tensor[index]+=1.\n",
    "    \n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X = torch.cat([Variable(make_BoW(x,word2index)).view(1,-1) for x in train_X])\n",
    "train_y = torch.cat([Variable(torch.LongTensor([class2index[y]])) for y in train_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 53])\n"
     ]
    }
   ],
   "source": [
    "print(train_X.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BoWClassifier(nn.Module):\n",
    "    def __init__(self,vocab_size,output_size):\n",
    "        super(BoWClassifier,self).__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(vocab_size,output_size)\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        return self.linear(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STEP = 100\n",
    "LR = 0.1\n",
    "model = BoWClassifier(len(word2index),2)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(),lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7569531202316284\n",
      "0.5769873857498169\n",
      "0.45888298749923706\n",
      "0.37734800577163696\n",
      "0.3184089958667755\n",
      "0.2741793394088745\n",
      "0.23996441066265106\n",
      "0.21282777190208435\n",
      "0.19085267186164856\n",
      "0.17274217307567596\n"
     ]
    }
   ],
   "source": [
    "for step in range(STEP):\n",
    "    model.zero_grad()\n",
    "    preds = model(train_X)\n",
    "    loss = loss_function(preds,train_y)\n",
    "    if step % 10 == 0:\n",
    "        print(loss.data[0])\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index2class = {v:k for k,v in class2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : 쭈꾸미 맛집 좀 찾아줘\n",
      "Prediction : FOOD\n",
      "Truth : FOOD\n",
      "\n",
      "\n",
      "Input : 매콤한 떡볶이 먹고싶다\n",
      "Prediction : FOOD\n",
      "Truth : FOOD\n",
      "\n",
      "\n",
      "Input : 강남 씨지비 조조 영화 스케줄표 좀\n",
      "Prediction : MEDIA\n",
      "Truth : MEDIA\n",
      "\n",
      "\n",
      "Input : 효리네 민박 보고싶엉\n",
      "Prediction : MEDIA\n",
      "Truth : MEDIA\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for test in test_data:\n",
    "    X = kor_tagger.morphs(test[0])\n",
    "    X = Variable(make_BoW(X,word2index)).view(1,-1)\n",
    "    \n",
    "    pred = model(X)\n",
    "    pred = pred.max(1)[1].data[0]\n",
    "    print(\"Input : %s\" % test[0])\n",
    "    print(\"Prediction : %s\" % index2class[pred])\n",
    "    print(\"Truth : %s\" % test[1])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
