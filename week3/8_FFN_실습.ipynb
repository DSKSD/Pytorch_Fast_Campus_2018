{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc67838c050>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.utils.data as torchdata\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as vdatasets\n",
    "import torchvision.utils as vutils\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. MNIST 데이터를 Train, Test 둘 다 로딩하고 데이터 로더를 만든다. (데이터의 path는 \"../../data/MNIST\")\n",
    "2. FFN(Feed forward network)를 모델링하는데\n",
    "  - 데이터의 Input 차원은 784(28*28), Output 차원은 10\n",
    "  - 히든 레이어를 2개를 사용하는데 각각의 차원은 512, 512\n",
    "  - Activation function은 tanh를 사용\n",
    "  - Xavier_normal을 이용해서 weight를 초기화하고, bias는 0.1로 초기화\n",
    "  - Dropout을 마지막 아웃풋을 제외하고 적용한다. (drop probability는 0.3)\n",
    "3. Optimizer는 SGD을 사용하고 learning rate는 각자가 적절한 값으로 사용, 또한 0.00001만큼의 weight decay를 준다\n",
    "4. EPOCH = 10, BATCH_SIZE = 32\n",
    "4. test 데이터셋을 사용해서 accuracy를 측정해본다\n",
    "5. 5번에서 찍어본 accuracy 보다 더 좋은 accuracy를 얻기 위해 위의 설정들을 바꿔본다(더 높은 accuracy를 얻는다면 성공)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset = vdatasets.MNIST(root='../../data/MNIST/',\n",
    "                               train=True, \n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           shuffle=True,\n",
    "                                           num_workers=2,\n",
    "                                           drop_last=True) # 이동평균이 튀는걸 방지\n",
    "\n",
    "test_dataset = vdatasets.MNIST(root='../../data/MNIST/',\n",
    "                               train=False, \n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           shuffle=True,\n",
    "                                           num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    \n",
    "    def __init__(self,hidden_size):\n",
    "        super(FFN,self).__init__()\n",
    "        \n",
    "        self.l1 = nn.Linear(784,hidden_size)\n",
    "        self.l2 = nn.Linear(hidden_size,hidden_size)\n",
    "        self.l3 = nn.Linear(hidden_size,10)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def init_weight(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                param.data = nn.init.xavier_normal(param.data)\n",
    "            else:\n",
    "                param.data = nn.init.constant(param.data,0.1)\n",
    "                \n",
    "    def forward(self,inputs):\n",
    "        outputs = self.activation(self.l1(inputs))\n",
    "        outputs = self.dropout(outputs)\n",
    "        outputs = self.activation(self.l2(outputs))\n",
    "        outputs = self.dropout(outputs)\n",
    "        \n",
    "        return self.l3(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCH = 10\n",
    "LR=0.001\n",
    "LAMDA = 0.00001\n",
    "\n",
    "model = FFN(512)\n",
    "model.init_weight()\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(),lr=LR,weight_decay=LAMDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10] mean_loss : 1.323\n",
      "[1/10] mean_loss : 0.673\n",
      "[2/10] mean_loss : 0.536\n",
      "[3/10] mean_loss : 0.473\n",
      "[4/10] mean_loss : 0.436\n",
      "[5/10] mean_loss : 0.413\n",
      "[6/10] mean_loss : 0.396\n",
      "[7/10] mean_loss : 0.382\n",
      "[8/10] mean_loss : 0.373\n",
      "[9/10] mean_loss : 0.365\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(EPOCH):\n",
    "    losses=[]\n",
    "    for i,(inputs,targets) in enumerate(train_loader):\n",
    "        inputs,targets = Variable(inputs.view(-1,784)), Variable(targets)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        preds = model(inputs)\n",
    "        loss = loss_function(preds,targets)\n",
    "        losses.append(loss.data[0])\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(\"[%d/%d] mean_loss : %.3f\" % (epoch,EPOCH,np.mean(losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy 측정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy :  0.9101\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "num_hit=0\n",
    "for i,(inputs,targets) in enumerate(test_loader):\n",
    "    inputs, targets = Variable(inputs).view(-1,784), Variable(targets)\n",
    "    model.zero_grad()\n",
    "    preds = model(inputs)\n",
    "    preds = preds.max(1)[1]\n",
    "    num_hit += torch.eq(preds,targets).sum().data[0]\n",
    "    \n",
    "print(\"accuracy : \",num_hit / len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
