{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as vdatasets\n",
    "import torchvision.utils as vutils\n",
    "import torchvision.models as vmodels\n",
    "import torchvision\n",
    "import random\n",
    "from PIL import Image\n",
    "import json\n",
    "import Augmentor\n",
    "torch.manual_seed(1)\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base = {\n",
    "    '300': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'C', 512, 512, 512, 'M',\n",
    "            512, 512, 512],\n",
    "    '512': [],\n",
    "}\n",
    "extras = {\n",
    "    '300': [256, 'S', 512, 128, 'S', 256, 128, 256, 128, 256],\n",
    "    '512': [],\n",
    "}\n",
    "mbox = {\n",
    "    '300': [4, 6, 6, 6, 4, 4],  # number of boxes per feature map location\n",
    "    '512': [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vgg(cfg, i, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = i\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        elif v == 'C':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "    conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\n",
    "    conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n",
    "    layers += [pool5, conv6,\n",
    "               nn.ReLU(inplace=True), conv7, nn.ReLU(inplace=True)]\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg = vgg(base[str(300)], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Conv2d (3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace),\n",
       " Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace),\n",
       " MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1)),\n",
       " Conv2d (64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace),\n",
       " Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace),\n",
       " MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1)),\n",
       " Conv2d (128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace),\n",
       " Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace),\n",
       " Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace),\n",
       " MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1)),\n",
       " Conv2d (256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace),\n",
       " Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace),\n",
       " Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace),\n",
       " MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1)),\n",
       " Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace),\n",
       " Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace),\n",
       " Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace),\n",
       " MaxPool2d(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(1, 1)),\n",
       " Conv2d (512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6)),\n",
       " ReLU(inplace),\n",
       " Conv2d (1024, 1024, kernel_size=(1, 1), stride=(1, 1)),\n",
       " ReLU(inplace)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_extras(cfg, i, batch_norm=False):\n",
    "    # Extra layers added to VGG for feature scaling\n",
    "    layers = []\n",
    "    in_channels = i\n",
    "    flag = False\n",
    "    for k, v in enumerate(cfg):\n",
    "        if in_channels != 'S':\n",
    "            if v == 'S':\n",
    "                layers += [nn.Conv2d(in_channels, cfg[k + 1],\n",
    "                           kernel_size=(1, 3)[flag], stride=2, padding=1)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, v, kernel_size=(1, 3)[flag])]\n",
    "            flag = not flag\n",
    "        in_channels = v\n",
    "    return layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extra = add_extras(extras[str(300)], 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Conv2d (1024, 256, kernel_size=(1, 1), stride=(1, 1)),\n",
       " Conv2d (256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)),\n",
       " Conv2d (512, 128, kernel_size=(1, 1), stride=(1, 1)),\n",
       " Conv2d (128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)),\n",
       " Conv2d (256, 128, kernel_size=(1, 1), stride=(1, 1)),\n",
       " Conv2d (128, 256, kernel_size=(3, 3), stride=(1, 1)),\n",
       " Conv2d (256, 128, kernel_size=(1, 1), stride=(1, 1)),\n",
       " Conv2d (128, 256, kernel_size=(3, 3), stride=(1, 1))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multibox(vgg, extra_layers, cfg, num_classes):\n",
    "    loc_layers = []\n",
    "    conf_layers = []\n",
    "    vgg_source = [21, -2]\n",
    "    for k, v in enumerate(vgg_source):\n",
    "        loc_layers += [nn.Conv2d(vgg[v].out_channels,\n",
    "                                 cfg[k] * 4, kernel_size=3, padding=1)]\n",
    "        conf_layers += [nn.Conv2d(vgg[v].out_channels,\n",
    "                        cfg[k] * num_classes, kernel_size=3, padding=1)]\n",
    "    for k, v in enumerate(extra_layers[1::2], 2):\n",
    "        loc_layers += [nn.Conv2d(v.out_channels, cfg[k]\n",
    "                                 * 4, kernel_size=3, padding=1)]\n",
    "        conf_layers += [nn.Conv2d(v.out_channels, cfg[k]\n",
    "                                  * num_classes, kernel_size=3, padding=1)]\n",
    "    return vgg, extra_layers, (loc_layers, conf_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_,extras_,head_=multibox(vgg,\n",
    "                                 extra,\n",
    "                                 mbox[str(300)], 21) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Conv2d (3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace),\n",
       " Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace),\n",
       " MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1)),\n",
       " Conv2d (64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace),\n",
       " Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace),\n",
       " MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1)),\n",
       " Conv2d (128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace),\n",
       " Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace),\n",
       " Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace),\n",
       " MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1)),\n",
       " Conv2d (256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace),\n",
       " Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace),\n",
       " Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace),\n",
       " MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1)),\n",
       " Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace),\n",
       " Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace),\n",
       " Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace),\n",
       " MaxPool2d(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(1, 1)),\n",
       " Conv2d (512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6)),\n",
       " ReLU(inplace),\n",
       " Conv2d (1024, 1024, kernel_size=(1, 1), stride=(1, 1)),\n",
       " ReLU(inplace)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Conv2d (3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace),\n",
       " Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace),\n",
       " MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1)),\n",
       " Conv2d (64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace),\n",
       " Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace),\n",
       " MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1)),\n",
       " Conv2d (128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace),\n",
       " Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace),\n",
       " Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace),\n",
       " MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1)),\n",
       " Conv2d (256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace),\n",
       " Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace),\n",
       " Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg[:23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1)),\n",
       " Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace),\n",
       " Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace),\n",
       " Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace),\n",
       " MaxPool2d(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(1, 1)),\n",
       " Conv2d (512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6)),\n",
       " ReLU(inplace),\n",
       " Conv2d (1024, 1024, kernel_size=(1, 1), stride=(1, 1)),\n",
       " ReLU(inplace)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg[23:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Conv2d (1024, 256, kernel_size=(1, 1), stride=(1, 1)),\n",
       " Conv2d (256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)),\n",
       " Conv2d (512, 128, kernel_size=(1, 1), stride=(1, 1)),\n",
       " Conv2d (128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)),\n",
       " Conv2d (256, 128, kernel_size=(1, 1), stride=(1, 1)),\n",
       " Conv2d (128, 256, kernel_size=(3, 3), stride=(1, 1)),\n",
       " Conv2d (256, 128, kernel_size=(1, 1), stride=(1, 1)),\n",
       " Conv2d (128, 256, kernel_size=(3, 3), stride=(1, 1))]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Conv2d (512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       "  Conv2d (1024, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       "  Conv2d (512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       "  Conv2d (256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       "  Conv2d (256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       "  Conv2d (256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))],\n",
       " [Conv2d (512, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       "  Conv2d (1024, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       "  Conv2d (512, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       "  Conv2d (256, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       "  Conv2d (256, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       "  Conv2d (256, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 디폴트 박스 생성기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v2 = {\n",
    "    'feature_maps' : [38, 19, 10, 5, 3, 1],\n",
    "\n",
    "    'min_dim' : 300,\n",
    "\n",
    "    'steps' : [8, 16, 32, 64, 100, 300],\n",
    "\n",
    "    'min_sizes' : [30, 60, 111, 162, 213, 264],\n",
    "\n",
    "    'max_sizes' : [60, 111, 162, 213, 264, 315],\n",
    "\n",
    "    # 'aspect_ratios' : [[2, 1/2], [2, 1/2, 3, 1/3], [2, 1/2, 3, 1/3],\n",
    "    #                    [2, 1/2, 3, 1/3], [2, 1/2], [2, 1/2]],\n",
    "    'aspect_ratios' : [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
    "\n",
    "    'variance' : [0.1, 0.2],\n",
    "\n",
    "    'clip' : True,\n",
    "\n",
    "    'name' : 'v2',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt as sqrt\n",
    "from itertools import product as product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.0133  0.0133  0.1000  0.1000\n",
       " 0.0133  0.0133  0.1414  0.1414\n",
       " 0.0133  0.0133  0.1414  0.0707\n",
       "               ⋮                \n",
       " 0.5000  0.5000  0.9612  0.9612\n",
       " 0.5000  0.5000  1.0000  0.6223\n",
       " 0.5000  0.5000  0.6223  1.0000\n",
       "[torch.FloatTensor of size 8732x4]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = []\n",
    "\n",
    "for k, f in enumerate(v2['feature_maps']):\n",
    "    for i, j in product(range(f), repeat=2):\n",
    "        f_k = 300 / v2['steps'][k]\n",
    "        # unit center x,y\n",
    "        cx = (j + 0.5) / f_k\n",
    "        cy = (i + 0.5) / f_k\n",
    "\n",
    "        # aspect_ratio: 1\n",
    "        # rel size: min_size\n",
    "        s_k = v2['min_sizes'][k]/300\n",
    "        mean += [cx, cy, s_k, s_k]\n",
    "\n",
    "        # aspect_ratio: 1\n",
    "        # rel size: sqrt(s_k * s_(k+1))\n",
    "        s_k_prime = sqrt(s_k * (v2['max_sizes'][k]/300))\n",
    "        mean += [cx, cy, s_k_prime, s_k_prime]\n",
    "\n",
    "        # rest of aspect ratios\n",
    "        for ar in v2['aspect_ratios'][k]:\n",
    "            mean += [cx, cy, s_k*sqrt(ar), s_k/sqrt(ar)]\n",
    "            mean += [cx, cy, s_k/sqrt(ar), s_k*sqrt(ar)]\n",
    "            \n",
    "output = torch.Tensor(mean).view(-1, 4)\n",
    "output.clamp_(max=1, min=0)\n",
    "output = Variable(output, volatile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "priors = output[:loc_data.size(1), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SSD(nn.Module):`\n",
    "    \"\"\"Single Shot Multibox Architecture\n",
    "    The network is composed of a base VGG network followed by the\n",
    "    added multibox conv layers.  Each multibox layer branches into\n",
    "        1) conv2d for class conf scores\n",
    "        2) conv2d for localization predictions\n",
    "        3) associated priorbox layer to produce default bounding\n",
    "           boxes specific to the layer's feature map size.\n",
    "    See: https://arxiv.org/pdf/1512.02325.pdf for more details.\n",
    "    Args:\n",
    "        phase: (string) Can be \"test\" or \"train\"\n",
    "        base: VGG16 layers for input, size of either 300 or 500\n",
    "        extras: extra layers that feed to multibox loc and conf layers\n",
    "        head: \"multibox head\" consists of loc and conf conv layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, phase, base, extras, head, num_classes):\n",
    "        super(SSD, self).__init__()\n",
    "        self.phase = phase\n",
    "        self.num_classes = num_classes\n",
    "        # TODO: implement __call__ in PriorBox\n",
    "        self.priorbox = PriorBox(v2)\n",
    "        self.priors = Variable(self.priorbox.forward(), volatile=True)\n",
    "        self.size = 300\n",
    "\n",
    "        # SSD network\n",
    "        self.vgg = nn.ModuleList(base)\n",
    "        # Layer learns to scale the l2 normalized features from conv4_3\n",
    "        self.L2Norm = L2Norm(512, 20)\n",
    "        self.extras = nn.ModuleList(extras)\n",
    "\n",
    "        self.loc = nn.ModuleList(head[0])\n",
    "        self.conf = nn.ModuleList(head[1])\n",
    "\n",
    "        if phase == 'test':\n",
    "            self.softmax = nn.Softmax()\n",
    "            self.detect = Detect(num_classes, 0, 200, 0.01, 0.45)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Applies network layers and ops on input image(s) x.\n",
    "        Args:\n",
    "            x: input image or batch of images. Shape: [batch,3,300,300].\n",
    "        Return:\n",
    "            Depending on phase:\n",
    "            test:\n",
    "                Variable(tensor) of output class label predictions,\n",
    "                confidence score, and corresponding location predictions for\n",
    "                each object detected. Shape: [batch,topk,7]\n",
    "            train:\n",
    "                list of concat outputs from:\n",
    "                    1: confidence layers, Shape: [batch*num_priors,num_classes]\n",
    "                    2: localization layers, Shape: [batch,num_priors*4]\n",
    "                    3: priorbox layers, Shape: [2,num_priors*4]\n",
    "        \"\"\"\n",
    "        sources = list()\n",
    "        loc = list()\n",
    "        conf = list()\n",
    "\n",
    "        # apply vgg up to conv4_3 relu\n",
    "        for k in range(23):\n",
    "            x = self.vgg[k](x)\n",
    "\n",
    "        s = self.L2Norm(x)\n",
    "        sources.append(s)\n",
    "\n",
    "        # apply vgg up to fc7\n",
    "        for k in range(23, len(self.vgg)):\n",
    "            x = self.vgg[k](x)\n",
    "        sources.append(x)\n",
    "\n",
    "        # apply extra layers and cache source layer outputs\n",
    "        for k, v in enumerate(self.extras):\n",
    "            x = F.relu(v(x), inplace=True)\n",
    "            if k % 2 == 1:\n",
    "                sources.append(x)\n",
    "\n",
    "        # apply multibox head to source layers\n",
    "        for (x, l, c) in zip(sources, self.loc, self.conf):\n",
    "            loc.append(l(x).permute(0, 2, 3, 1).contiguous())\n",
    "            conf.append(c(x).permute(0, 2, 3, 1).contiguous())\n",
    "\n",
    "        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n",
    "        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)\n",
    "        if self.phase == \"test\":\n",
    "            output = self.detect(\n",
    "                loc.view(loc.size(0), -1, 4),                   # loc preds\n",
    "                self.softmax(conf.view(-1, self.num_classes)),  # conf preds\n",
    "                self.priors.type(type(x.data))                  # default boxes\n",
    "            )\n",
    "        else:\n",
    "            output = (\n",
    "                loc.view(loc.size(0), -1, 4),\n",
    "                conf.view(conf.size(0), -1, self.num_classes),\n",
    "                self.priors\n",
    "            )\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 멀티박스 로스 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultiBoxLoss(nn.Module):\n",
    "    \"\"\"SSD Weighted Loss Function\n",
    "    Compute Targets:\n",
    "        1) Produce Confidence Target Indices by matching  ground truth boxes\n",
    "           with (default) 'priorboxes' that have jaccard index > threshold parameter\n",
    "           (default threshold: 0.5).\n",
    "        2) Produce localization target by 'encoding' variance into offsets of ground\n",
    "           truth boxes and their matched  'priorboxes'.\n",
    "        3) Hard negative mining to filter the excessive number of negative examples\n",
    "           that comes with using a large number of default bounding boxes.\n",
    "           (default negative:positive ratio 3:1)\n",
    "    Objective Loss:\n",
    "        L(x,c,l,g) = (Lconf(x, c) + αLloc(x,l,g)) / N\n",
    "        Where, Lconf is the CrossEntropy Loss and Lloc is the SmoothL1 Loss\n",
    "        weighted by α which is set to 1 by cross val.\n",
    "        Args:\n",
    "            c: class confidences,\n",
    "            l: predicted boxes,\n",
    "            g: ground truth boxes\n",
    "            N: number of matched default boxes\n",
    "        See: https://arxiv.org/pdf/1512.02325.pdf for more details.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, overlap_thresh, prior_for_matching,\n",
    "                 bkg_label, neg_mining, neg_pos, neg_overlap, encode_target,\n",
    "                 use_gpu=True):\n",
    "        super(MultiBoxLoss, self).__init__()\n",
    "        self.use_gpu = use_gpu\n",
    "        self.num_classes = num_classes\n",
    "        self.threshold = overlap_thresh\n",
    "        self.background_label = bkg_label\n",
    "        self.encode_target = encode_target\n",
    "        self.use_prior_for_matching = prior_for_matching\n",
    "        self.do_neg_mining = neg_mining\n",
    "        self.negpos_ratio = neg_pos\n",
    "        self.neg_overlap = neg_overlap\n",
    "        self.variance = cfg['variance']\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        \"\"\"Multibox Loss\n",
    "        Args:\n",
    "            predictions (tuple): A tuple containing loc preds, conf preds,\n",
    "            and prior boxes from SSD net.\n",
    "                conf shape: torch.size(batch_size,num_priors,num_classes)\n",
    "                loc shape: torch.size(batch_size,num_priors,4)\n",
    "                priors shape: torch.size(num_priors,4)\n",
    "            ground_truth (tensor): Ground truth boxes and labels for a batch,\n",
    "                shape: [batch_size,num_objs,5] (last idx is the label).\n",
    "        \"\"\"\n",
    "        loc_data, conf_data, priors = predictions\n",
    "        num = loc_data.size(0)\n",
    "        priors = priors[:loc_data.size(1), :]\n",
    "        num_priors = (priors.size(0))\n",
    "        num_classes = self.num_classes\n",
    "\n",
    "        # match priors (default boxes) and ground truth boxes\n",
    "        loc_t = torch.Tensor(num, num_priors, 4)\n",
    "        conf_t = torch.LongTensor(num, num_priors)\n",
    "        for idx in range(num):\n",
    "            truths = targets[idx][:, :-1].data\n",
    "            labels = targets[idx][:, -1].data\n",
    "            defaults = priors.data\n",
    "            match(self.threshold, truths, defaults, self.variance, labels,\n",
    "                  loc_t, conf_t, idx)\n",
    "        if self.use_gpu:\n",
    "            loc_t = loc_t.cuda()\n",
    "            conf_t = conf_t.cuda()\n",
    "        # wrap targets\n",
    "        loc_t = Variable(loc_t, requires_grad=False)\n",
    "        conf_t = Variable(conf_t, requires_grad=False)\n",
    "\n",
    "        pos = conf_t > 0\n",
    "        num_pos = pos.sum(dim=1, keepdim=True)\n",
    "\n",
    "        # Localization Loss (Smooth L1)\n",
    "        # Shape: [batch,num_priors,4]\n",
    "        pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data)\n",
    "        loc_p = loc_data[pos_idx].view(-1, 4)\n",
    "        loc_t = loc_t[pos_idx].view(-1, 4)\n",
    "        loss_l = F.smooth_l1_loss(loc_p, loc_t, size_average=False)\n",
    "\n",
    "        # Compute max conf across batch for hard negative mining\n",
    "        batch_conf = conf_data.view(-1, self.num_classes)\n",
    "\n",
    "        loss_c = log_sum_exp(batch_conf) - batch_conf.gather(1, conf_t.view(-1, 1))\n",
    "\n",
    "        # Hard Negative Mining\n",
    "        loss_c[pos] = 0  # filter out pos boxes for now\n",
    "        loss_c = loss_c.view(num, -1)\n",
    "        _, loss_idx = loss_c.sort(1, descending=True)\n",
    "        _, idx_rank = loss_idx.sort(1)\n",
    "        num_pos = pos.long().sum(1, keepdim=True)\n",
    "        num_neg = torch.clamp(self.negpos_ratio*num_pos, max=pos.size(1)-1)\n",
    "        neg = idx_rank < num_neg.expand_as(idx_rank)\n",
    "\n",
    "        # Confidence Loss Including Positive and Negative Examples\n",
    "        pos_idx = pos.unsqueeze(2).expand_as(conf_data)\n",
    "        neg_idx = neg.unsqueeze(2).expand_as(conf_data)\n",
    "        conf_p = conf_data[(pos_idx+neg_idx).gt(0)].view(-1, self.num_classes)\n",
    "        targets_weighted = conf_t[(pos+neg).gt(0)]\n",
    "        loss_c = F.cross_entropy(conf_p, targets_weighted, size_average=False)\n",
    "\n",
    "        # Sum of losses: L(x,c,l,g) = (Lconf(x, c) + αLloc(x,l,g)) / N\n",
    "\n",
    "        N = num_pos.data.sum()\n",
    "        loss_l /= N\n",
    "        loss_c /= N\n",
    "        return loss_l, loss_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = MultiBoxLoss(num_classes, 0.5, True, 0, True, 3, 0.5, False, args.cuda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
