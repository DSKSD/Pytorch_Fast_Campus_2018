{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as vmodels\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import nltk\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 로딩 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.39s)\n",
      "creating index...\n",
      "index created!\n",
      "Number of samples:  40504\n",
      "Image Size:  torch.Size([3, 224, 224])\n",
      "['A group of men sitting around a table eating food.', 'Men sitting at a table smiling while eating food.', 'A group of people enjoying a pot-luck dinner.', 'A person at a table with some food.', 'Two men sampling chili and beer in a tent.']\n"
     ]
    }
   ],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "cap = dset.CocoCaptions(root = '../../data/CocoCaption/val2014/',\n",
    "                        annFile = '../../data/CocoCaption/captions_val2014.json',\n",
    "                        transform=transforms.Compose([transforms.ToTensor(),normalize])\n",
    "\n",
    "print('Number of samples: ', len(cap))\n",
    "img, target = cap[3] # load 4th sample\n",
    "\n",
    "print(\"Image Size: \", img.size())\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(cap,batch_size=3,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sents=[]\n",
    "for value in cap.coco.anns.values():\n",
    "    sents.append(nltk.word_tokenize(value['caption']))\n",
    "    \n",
    "vocab = list(set([w for s in sents for w in s]))\n",
    "\n",
    "word2index={'<pad>' : 0, '<unk>' : 1, '<s>' : 2, '</s>' :3}\n",
    "for vo in vocab:\n",
    "    if word2index.get(vo)==None:\n",
    "        word2index[vo]=len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_index):\n",
    "    idxs = list(map(lambda w: to_index[w] if w in to_index.keys() else to_index[\"<unk>\"], seq))\n",
    "    return Variable(torch.LongTensor(idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_batch(batch,word2index):\n",
    "    x,y = batch\n",
    "    x = torch.cat([xx.unsqueeze(0) for xx in x])\n",
    "    y = random.choice(y)\n",
    "    y = [prepare_sequence(nltk.word_tokenize(yy),word2index).view(1,-1) for yy in y]\n",
    "    max_y = max([s.size(1) for s in y])\n",
    "    y_p=[]\n",
    "    for i in range(len(y)):\n",
    "        if y[i].size(1)<max_y:\n",
    "            y_p.append(torch.cat([y[i],Variable(torch.LongTensor([word2index['<pad>']]*(max_y-y[i].size(1)))).view(1,-1)],1))\n",
    "        else:\n",
    "            y_p.append(y[i])\n",
    "        \n",
    "    y = torch.cat(y_p)\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x,y = prepare_batch(batch,word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGGNET16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vggnet = vmodels.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_extractor = nn.Sequential(*(list(vggnet.features)[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature = feature_extractor(Variable(img.unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature = feature.view(1,512,196).transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from attention import Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,V,E,H,sos_idx,max_len=15):\n",
    "        super(Decoder,self).__init__()\n",
    "        \n",
    "        self.hidden_size = H\n",
    "        self.max_len = max_len\n",
    "        self.sos_idx = sos_idx\n",
    "        self.init_F = nn.Linear(H,H)\n",
    "        self.embed = nn.Embedding(V,E)\n",
    "        self.gru = nn.GRU(E+H,H,batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.linear = nn.Linear(2*H,V)\n",
    "        self.attention = Attention(H,'general') # 어텐션\n",
    "        \n",
    "    def start_token(self,batch_size):\n",
    "        sos = Variable(torch.LongTensor([self.sos_idx]*batch_size)).unsqueeze(1)\n",
    "        if USE_CUDA: sos = sos.cuda()\n",
    "        return sos\n",
    "       \n",
    "    def init_hidden(self,encoder_hiddens):\n",
    "        mean_hidden = torch.mean(encoder_hiddens,1)\n",
    "        hidden = self.init_F(mean_hidden) # B,H\n",
    "        return hidden.unsqueeze(0) # 1,B,H\n",
    "        \n",
    "    def forward(self,encoder_hiddens, max_len=None):\n",
    "        \"\"\"\n",
    "        encoder_hiddens : B,196,512 (CNN에서 뽑아낸 Visual features)\n",
    "        \"\"\"\n",
    "        if max_len is None: max_len = self.max_len\n",
    "        batch_size = encoder_hiddens.size(0)\n",
    "        \n",
    "        inputs = self.start_token(batch_size) # Batch_size\n",
    "        hidden = self.init_hidden(encoder_hiddens)\n",
    "        embed = self.embed(inputs)\n",
    "#         embed= self.dropout(embed)\n",
    "        scores=[]\n",
    "        attn_weights=[]\n",
    "        for _ in range(max_len):\n",
    "            \n",
    "            # context vector 계산\n",
    "            context = self.attention(hidden.transpose(0,1), encoder_hiddens)\n",
    "#             attn_weights.append(attn_weight.squeeze(1))\n",
    "            \n",
    "            # concat해서 rnn에\n",
    "            rnn_input = torch.cat([embed,context],2)\n",
    "            _, hidden = self.gru(rnn_input,hidden)\n",
    "            \n",
    "            # concat해서 linear에\n",
    "            concated = torch.cat([hidden.transpose(0,1),context],2)\n",
    "            score = self.linear(concated.squeeze(1))\n",
    "            scores.append(score)\n",
    "            decoded = score.max(1)[1] # greedy\n",
    "            embed = self.embed(decoded).unsqueeze(1) # y_{t-1}\n",
    "#             embed = self.dropout(embed)\n",
    "            \n",
    "        #  column-wise concat, reshape!!\n",
    "        scores = torch.cat(scores,1)\n",
    "        return scores.view(batch_size*max_len,-1) #, torch.cat(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decoder = Decoder(len(word2index),100,512,word2index['<s>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    x,y = prepare_batch(batch,word2index)\n",
    "    feature_extractor.zero_grad()\n",
    "    decoder.zero_grad()\n",
    "    features = feature_extractor(Variable(x))\n",
    "    features = features.view(-1,512,196).transpose(1,2)\n",
    "    preds = decoder(features,y.size(1))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
