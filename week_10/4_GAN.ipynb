{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as vdatasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "DATA_PATH = os.environ['DATA_PATH']\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/soumith/ganhacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper Parameters \n",
    "INPUT_SIZE = 784\n",
    "HIDDEN_SIZE = 256\n",
    "LATENT_SIZE = 100\n",
    "EPOCH = 200\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def denorm(x):\n",
    "    out = (x + 1) / 2\n",
    "    return out.clamp(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MNIST Dataset (Images and Labels)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=(0.5, 0.5, 0.5), \n",
    "                                     std=(0.5, 0.5, 0.5))])\n",
    "\n",
    "# MNIST Dataset (Images and Labels)\n",
    "train_dataset = vdatasets.MNIST(root=DATA_PATH+'MNIST/', \n",
    "                            train=True, \n",
    "                            transform=transform,\n",
    "                            download=True)\n",
    "\n",
    "# Dataset Loader (Input Pipline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Generator = nn.Sequential(nn.Linear(LATENT_SIZE,HIDDEN_SIZE),\n",
    "                                        nn.LeakyReLU(0.2),\n",
    "                                        nn.Linear(HIDDEN_SIZE,HIDDEN_SIZE),\n",
    "                                        nn.LeakyReLU(0.2),\n",
    "                                        nn.Linear(HIDDEN_SIZE,INPUT_SIZE),\n",
    "                                        nn.Tanh())\n",
    "\n",
    "Discriminator = nn.Sequential(nn.Linear(INPUT_SIZE,HIDDEN_SIZE),\n",
    "                                             nn.LeakyReLU(0.2),\n",
    "                                             nn.Linear(HIDDEN_SIZE,HIDDEN_SIZE),\n",
    "                                             nn.LeakyReLU(0.2),\n",
    "                                             nn.Linear(HIDDEN_SIZE,1),\n",
    "                                             nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LR = 0.0002\n",
    "\n",
    "loss_function = nn.BCELoss()  \n",
    "d_optimizer = optim.Adam(Discriminator.parameters(), lr=LR)\n",
    "g_optimizer = optim.Adam(Generator.parameters(),lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/200], Step: [300/600], D Loss: 1.1531, G Loss: 1.5152\n",
      "Epoch: [1/200], Step: [600/600], D Loss: 0.2564, G Loss: 2.8510\n",
      "Epoch: [2/200], Step: [300/600], D Loss: 0.0152, G Loss: 5.4922\n",
      "Epoch: [2/200], Step: [600/600], D Loss: 1.3797, G Loss: 2.3368\n",
      "Epoch: [3/200], Step: [300/600], D Loss: 0.4456, G Loss: 2.7325\n",
      "Epoch: [3/200], Step: [600/600], D Loss: 0.3635, G Loss: 2.9107\n",
      "Epoch: [4/200], Step: [300/600], D Loss: 0.5053, G Loss: 2.6856\n",
      "Epoch: [4/200], Step: [600/600], D Loss: 1.1031, G Loss: 1.5296\n",
      "Epoch: [5/200], Step: [300/600], D Loss: 0.4972, G Loss: 2.4258\n",
      "Epoch: [5/200], Step: [600/600], D Loss: 0.5457, G Loss: 3.1082\n",
      "Epoch: [6/200], Step: [300/600], D Loss: 0.3753, G Loss: 3.6293\n",
      "Epoch: [6/200], Step: [600/600], D Loss: 0.2915, G Loss: 3.2657\n",
      "Epoch: [7/200], Step: [300/600], D Loss: 0.3205, G Loss: 4.4791\n",
      "Epoch: [7/200], Step: [600/600], D Loss: 0.5279, G Loss: 2.2808\n",
      "Epoch: [8/200], Step: [300/600], D Loss: 0.3635, G Loss: 4.2301\n",
      "Epoch: [8/200], Step: [600/600], D Loss: 0.3210, G Loss: 4.3745\n",
      "Epoch: [9/200], Step: [300/600], D Loss: 0.4249, G Loss: 3.2158\n",
      "Epoch: [9/200], Step: [600/600], D Loss: 0.3258, G Loss: 2.9580\n",
      "Epoch: [10/200], Step: [300/600], D Loss: 0.5114, G Loss: 3.7741\n",
      "Epoch: [10/200], Step: [600/600], D Loss: 0.5060, G Loss: 3.2297\n",
      "Epoch: [11/200], Step: [300/600], D Loss: 0.5869, G Loss: 2.6869\n",
      "Epoch: [11/200], Step: [600/600], D Loss: 0.4076, G Loss: 2.7410\n",
      "Epoch: [12/200], Step: [300/600], D Loss: 0.4091, G Loss: 3.0551\n",
      "Epoch: [12/200], Step: [600/600], D Loss: 0.2532, G Loss: 3.7446\n",
      "Epoch: [13/200], Step: [300/600], D Loss: 0.3946, G Loss: 3.1030\n",
      "Epoch: [13/200], Step: [600/600], D Loss: 0.7916, G Loss: 2.5168\n",
      "Epoch: [14/200], Step: [300/600], D Loss: 0.7352, G Loss: 3.0949\n",
      "Epoch: [14/200], Step: [600/600], D Loss: 0.4340, G Loss: 2.5096\n",
      "Epoch: [15/200], Step: [300/600], D Loss: 0.4453, G Loss: 2.7219\n",
      "Epoch: [15/200], Step: [600/600], D Loss: 0.5443, G Loss: 2.5534\n",
      "Epoch: [16/200], Step: [300/600], D Loss: 0.6924, G Loss: 2.9356\n",
      "Epoch: [16/200], Step: [600/600], D Loss: 0.4396, G Loss: 3.2116\n",
      "Epoch: [17/200], Step: [300/600], D Loss: 0.7136, G Loss: 2.5673\n",
      "Epoch: [17/200], Step: [600/600], D Loss: 0.4836, G Loss: 3.4452\n",
      "Epoch: [18/200], Step: [300/600], D Loss: 0.5716, G Loss: 3.2967\n",
      "Epoch: [18/200], Step: [600/600], D Loss: 0.4563, G Loss: 4.4294\n",
      "Epoch: [19/200], Step: [300/600], D Loss: 0.3871, G Loss: 2.8184\n",
      "Epoch: [19/200], Step: [600/600], D Loss: 0.5810, G Loss: 3.3604\n",
      "Epoch: [20/200], Step: [300/600], D Loss: 0.4766, G Loss: 3.0016\n",
      "Epoch: [20/200], Step: [600/600], D Loss: 0.5274, G Loss: 3.0664\n",
      "Epoch: [21/200], Step: [300/600], D Loss: 0.7089, G Loss: 1.6810\n",
      "Epoch: [21/200], Step: [600/600], D Loss: 0.5872, G Loss: 2.5613\n",
      "Epoch: [22/200], Step: [300/600], D Loss: 0.5764, G Loss: 1.9992\n",
      "Epoch: [22/200], Step: [600/600], D Loss: 0.9621, G Loss: 1.9449\n",
      "Epoch: [23/200], Step: [300/600], D Loss: 0.6076, G Loss: 2.1492\n",
      "Epoch: [23/200], Step: [600/600], D Loss: 0.6972, G Loss: 2.5213\n",
      "Epoch: [24/200], Step: [300/600], D Loss: 0.6694, G Loss: 1.9208\n",
      "Epoch: [24/200], Step: [600/600], D Loss: 0.7711, G Loss: 2.8342\n",
      "Epoch: [25/200], Step: [300/600], D Loss: 0.9635, G Loss: 1.5123\n",
      "Epoch: [25/200], Step: [600/600], D Loss: 0.7911, G Loss: 2.6449\n",
      "Epoch: [26/200], Step: [300/600], D Loss: 0.8144, G Loss: 1.4759\n",
      "Epoch: [26/200], Step: [600/600], D Loss: 0.9767, G Loss: 2.1372\n",
      "Epoch: [27/200], Step: [300/600], D Loss: 0.9379, G Loss: 2.5025\n",
      "Epoch: [27/200], Step: [600/600], D Loss: 0.7715, G Loss: 2.4622\n",
      "Epoch: [28/200], Step: [300/600], D Loss: 0.9097, G Loss: 2.0682\n",
      "Epoch: [28/200], Step: [600/600], D Loss: 0.8912, G Loss: 1.9380\n",
      "Epoch: [29/200], Step: [300/600], D Loss: 0.6129, G Loss: 2.0478\n",
      "Epoch: [29/200], Step: [600/600], D Loss: 0.7491, G Loss: 1.9616\n",
      "Epoch: [30/200], Step: [300/600], D Loss: 0.7338, G Loss: 1.9097\n",
      "Epoch: [30/200], Step: [600/600], D Loss: 0.7943, G Loss: 2.3144\n",
      "Epoch: [31/200], Step: [300/600], D Loss: 0.8872, G Loss: 2.0746\n",
      "Epoch: [31/200], Step: [600/600], D Loss: 0.8349, G Loss: 2.1759\n",
      "Epoch: [32/200], Step: [300/600], D Loss: 0.7782, G Loss: 2.0864\n",
      "Epoch: [32/200], Step: [600/600], D Loss: 0.6985, G Loss: 1.7066\n",
      "Epoch: [33/200], Step: [300/600], D Loss: 0.7269, G Loss: 2.3064\n",
      "Epoch: [33/200], Step: [600/600], D Loss: 0.7071, G Loss: 2.0629\n",
      "Epoch: [34/200], Step: [300/600], D Loss: 0.6313, G Loss: 2.5839\n",
      "Epoch: [34/200], Step: [600/600], D Loss: 0.9162, G Loss: 2.0057\n",
      "Epoch: [35/200], Step: [300/600], D Loss: 0.6278, G Loss: 2.5055\n",
      "Epoch: [35/200], Step: [600/600], D Loss: 0.7768, G Loss: 2.4597\n",
      "Epoch: [36/200], Step: [300/600], D Loss: 0.5912, G Loss: 2.2185\n",
      "Epoch: [36/200], Step: [600/600], D Loss: 0.8296, G Loss: 2.6794\n",
      "Epoch: [37/200], Step: [300/600], D Loss: 0.9894, G Loss: 2.2798\n",
      "Epoch: [37/200], Step: [600/600], D Loss: 0.9519, G Loss: 1.7878\n",
      "Epoch: [38/200], Step: [300/600], D Loss: 0.9725, G Loss: 1.4420\n",
      "Epoch: [38/200], Step: [600/600], D Loss: 0.9197, G Loss: 1.6899\n",
      "Epoch: [39/200], Step: [300/600], D Loss: 0.8493, G Loss: 2.1206\n",
      "Epoch: [39/200], Step: [600/600], D Loss: 1.0487, G Loss: 1.7742\n",
      "Epoch: [40/200], Step: [300/600], D Loss: 0.7394, G Loss: 2.0630\n",
      "Epoch: [40/200], Step: [600/600], D Loss: 0.8067, G Loss: 2.1061\n",
      "Epoch: [41/200], Step: [300/600], D Loss: 0.7885, G Loss: 1.4154\n",
      "Epoch: [41/200], Step: [600/600], D Loss: 0.9356, G Loss: 1.7313\n",
      "Epoch: [42/200], Step: [300/600], D Loss: 0.8619, G Loss: 1.9789\n",
      "Epoch: [42/200], Step: [600/600], D Loss: 0.6201, G Loss: 1.6451\n",
      "Epoch: [43/200], Step: [300/600], D Loss: 0.6188, G Loss: 1.7610\n",
      "Epoch: [43/200], Step: [600/600], D Loss: 0.7413, G Loss: 2.5501\n",
      "Epoch: [44/200], Step: [300/600], D Loss: 0.7009, G Loss: 1.6522\n",
      "Epoch: [44/200], Step: [600/600], D Loss: 0.7503, G Loss: 1.6801\n",
      "Epoch: [45/200], Step: [300/600], D Loss: 0.7441, G Loss: 1.4964\n",
      "Epoch: [45/200], Step: [600/600], D Loss: 0.8762, G Loss: 1.7792\n",
      "Epoch: [46/200], Step: [300/600], D Loss: 0.7197, G Loss: 2.7110\n",
      "Epoch: [46/200], Step: [600/600], D Loss: 0.8371, G Loss: 1.6338\n",
      "Epoch: [47/200], Step: [300/600], D Loss: 0.8291, G Loss: 1.9645\n",
      "Epoch: [47/200], Step: [600/600], D Loss: 0.6909, G Loss: 2.0763\n",
      "Epoch: [48/200], Step: [300/600], D Loss: 0.7273, G Loss: 2.0563\n",
      "Epoch: [48/200], Step: [600/600], D Loss: 0.6421, G Loss: 2.1111\n",
      "Epoch: [49/200], Step: [300/600], D Loss: 0.8966, G Loss: 1.4480\n",
      "Epoch: [49/200], Step: [600/600], D Loss: 1.0170, G Loss: 2.0470\n",
      "Epoch: [50/200], Step: [300/600], D Loss: 0.9723, G Loss: 1.8147\n",
      "Epoch: [50/200], Step: [600/600], D Loss: 0.9264, G Loss: 1.7447\n",
      "Epoch: [51/200], Step: [300/600], D Loss: 0.6763, G Loss: 2.2729\n",
      "Epoch: [51/200], Step: [600/600], D Loss: 0.6268, G Loss: 2.0284\n",
      "Epoch: [52/200], Step: [300/600], D Loss: 0.8824, G Loss: 2.0867\n",
      "Epoch: [52/200], Step: [600/600], D Loss: 0.7272, G Loss: 2.3737\n",
      "Epoch: [53/200], Step: [300/600], D Loss: 0.8700, G Loss: 2.0228\n",
      "Epoch: [53/200], Step: [600/600], D Loss: 0.6940, G Loss: 2.0315\n",
      "Epoch: [54/200], Step: [300/600], D Loss: 0.6468, G Loss: 2.3922\n",
      "Epoch: [54/200], Step: [600/600], D Loss: 0.7884, G Loss: 1.7167\n",
      "Epoch: [55/200], Step: [300/600], D Loss: 0.8099, G Loss: 1.8107\n",
      "Epoch: [55/200], Step: [600/600], D Loss: 1.0449, G Loss: 1.7814\n",
      "Epoch: [56/200], Step: [300/600], D Loss: 0.8288, G Loss: 1.8388\n",
      "Epoch: [56/200], Step: [600/600], D Loss: 0.8807, G Loss: 1.4196\n",
      "Epoch: [57/200], Step: [300/600], D Loss: 0.8475, G Loss: 1.7131\n",
      "Epoch: [57/200], Step: [600/600], D Loss: 0.9262, G Loss: 1.7603\n",
      "Epoch: [58/200], Step: [300/600], D Loss: 0.7564, G Loss: 1.6925\n",
      "Epoch: [58/200], Step: [600/600], D Loss: 0.7428, G Loss: 1.5423\n",
      "Epoch: [59/200], Step: [300/600], D Loss: 0.7702, G Loss: 2.0276\n",
      "Epoch: [59/200], Step: [600/600], D Loss: 0.7309, G Loss: 1.4246\n",
      "Epoch: [60/200], Step: [300/600], D Loss: 0.8461, G Loss: 1.3782\n",
      "Epoch: [60/200], Step: [600/600], D Loss: 0.7356, G Loss: 1.5518\n",
      "Epoch: [61/200], Step: [300/600], D Loss: 0.8337, G Loss: 2.0084\n",
      "Epoch: [61/200], Step: [600/600], D Loss: 0.7595, G Loss: 1.8093\n",
      "Epoch: [62/200], Step: [300/600], D Loss: 0.8499, G Loss: 1.7512\n",
      "Epoch: [62/200], Step: [600/600], D Loss: 0.8282, G Loss: 1.7311\n",
      "Epoch: [63/200], Step: [300/600], D Loss: 0.8393, G Loss: 1.5330\n",
      "Epoch: [63/200], Step: [600/600], D Loss: 0.7764, G Loss: 1.5914\n",
      "Epoch: [64/200], Step: [300/600], D Loss: 0.7763, G Loss: 1.9094\n",
      "Epoch: [64/200], Step: [600/600], D Loss: 0.9586, G Loss: 1.9892\n",
      "Epoch: [65/200], Step: [300/600], D Loss: 0.8064, G Loss: 2.2161\n",
      "Epoch: [65/200], Step: [600/600], D Loss: 0.8776, G Loss: 1.9115\n",
      "Epoch: [66/200], Step: [300/600], D Loss: 0.9248, G Loss: 1.6666\n",
      "Epoch: [66/200], Step: [600/600], D Loss: 0.7629, G Loss: 1.4142\n",
      "Epoch: [67/200], Step: [300/600], D Loss: 0.7466, G Loss: 1.4972\n",
      "Epoch: [67/200], Step: [600/600], D Loss: 0.9473, G Loss: 1.6238\n",
      "Epoch: [68/200], Step: [300/600], D Loss: 0.8201, G Loss: 1.8468\n",
      "Epoch: [68/200], Step: [600/600], D Loss: 0.7648, G Loss: 2.0784\n",
      "Epoch: [69/200], Step: [300/600], D Loss: 0.8125, G Loss: 1.7814\n",
      "Epoch: [69/200], Step: [600/600], D Loss: 0.8868, G Loss: 1.8206\n",
      "Epoch: [70/200], Step: [300/600], D Loss: 0.8368, G Loss: 1.7786\n",
      "Epoch: [70/200], Step: [600/600], D Loss: 0.7035, G Loss: 1.9764\n",
      "Epoch: [71/200], Step: [300/600], D Loss: 0.8046, G Loss: 2.0086\n",
      "Epoch: [71/200], Step: [600/600], D Loss: 0.8539, G Loss: 1.9564\n",
      "Epoch: [72/200], Step: [300/600], D Loss: 0.8110, G Loss: 2.0514\n",
      "Epoch: [72/200], Step: [600/600], D Loss: 0.8015, G Loss: 1.8011\n",
      "Epoch: [73/200], Step: [300/600], D Loss: 0.9573, G Loss: 2.2925\n",
      "Epoch: [73/200], Step: [600/600], D Loss: 0.8616, G Loss: 2.3351\n",
      "Epoch: [74/200], Step: [300/600], D Loss: 0.7094, G Loss: 1.8031\n",
      "Epoch: [74/200], Step: [600/600], D Loss: 0.6753, G Loss: 2.3254\n",
      "Epoch: [75/200], Step: [300/600], D Loss: 0.7837, G Loss: 1.7805\n",
      "Epoch: [75/200], Step: [600/600], D Loss: 0.9888, G Loss: 1.2803\n",
      "Epoch: [76/200], Step: [300/600], D Loss: 0.9459, G Loss: 2.0376\n",
      "Epoch: [76/200], Step: [600/600], D Loss: 0.6731, G Loss: 1.9027\n",
      "Epoch: [77/200], Step: [300/600], D Loss: 0.8138, G Loss: 1.7012\n",
      "Epoch: [77/200], Step: [600/600], D Loss: 0.9385, G Loss: 1.7526\n",
      "Epoch: [78/200], Step: [300/600], D Loss: 0.8949, G Loss: 1.9549\n",
      "Epoch: [78/200], Step: [600/600], D Loss: 0.8251, G Loss: 1.6295\n",
      "Epoch: [79/200], Step: [300/600], D Loss: 0.8124, G Loss: 1.8518\n",
      "Epoch: [79/200], Step: [600/600], D Loss: 0.7260, G Loss: 1.6390\n",
      "Epoch: [80/200], Step: [300/600], D Loss: 0.7382, G Loss: 1.7841\n",
      "Epoch: [80/200], Step: [600/600], D Loss: 0.7790, G Loss: 1.7245\n",
      "Epoch: [81/200], Step: [300/600], D Loss: 0.9022, G Loss: 1.3535\n",
      "Epoch: [81/200], Step: [600/600], D Loss: 0.8228, G Loss: 1.5226\n",
      "Epoch: [82/200], Step: [300/600], D Loss: 0.8217, G Loss: 1.9035\n",
      "Epoch: [82/200], Step: [600/600], D Loss: 0.8073, G Loss: 1.7754\n",
      "Epoch: [83/200], Step: [300/600], D Loss: 0.9108, G Loss: 1.7280\n",
      "Epoch: [83/200], Step: [600/600], D Loss: 0.8851, G Loss: 1.3015\n",
      "Epoch: [84/200], Step: [300/600], D Loss: 0.8564, G Loss: 2.1207\n",
      "Epoch: [84/200], Step: [600/600], D Loss: 0.8239, G Loss: 2.1355\n",
      "Epoch: [85/200], Step: [300/600], D Loss: 0.9291, G Loss: 2.0425\n",
      "Epoch: [85/200], Step: [600/600], D Loss: 0.8337, G Loss: 1.9075\n",
      "Epoch: [86/200], Step: [300/600], D Loss: 0.7082, G Loss: 1.8316\n",
      "Epoch: [86/200], Step: [600/600], D Loss: 0.9458, G Loss: 1.5971\n",
      "Epoch: [87/200], Step: [300/600], D Loss: 0.8143, G Loss: 1.8804\n",
      "Epoch: [87/200], Step: [600/600], D Loss: 0.8414, G Loss: 1.8945\n",
      "Epoch: [88/200], Step: [300/600], D Loss: 0.7341, G Loss: 2.1616\n",
      "Epoch: [88/200], Step: [600/600], D Loss: 0.8683, G Loss: 1.5285\n",
      "Epoch: [89/200], Step: [300/600], D Loss: 0.8919, G Loss: 1.4299\n",
      "Epoch: [89/200], Step: [600/600], D Loss: 0.8588, G Loss: 1.6502\n",
      "Epoch: [90/200], Step: [300/600], D Loss: 0.8196, G Loss: 1.8311\n",
      "Epoch: [90/200], Step: [600/600], D Loss: 0.9351, G Loss: 1.3442\n",
      "Epoch: [91/200], Step: [300/600], D Loss: 0.9793, G Loss: 1.8020\n",
      "Epoch: [91/200], Step: [600/600], D Loss: 0.7685, G Loss: 1.6387\n",
      "Epoch: [92/200], Step: [300/600], D Loss: 0.8686, G Loss: 1.7941\n",
      "Epoch: [92/200], Step: [600/600], D Loss: 0.8583, G Loss: 1.9575\n",
      "Epoch: [93/200], Step: [300/600], D Loss: 0.6504, G Loss: 1.8170\n",
      "Epoch: [93/200], Step: [600/600], D Loss: 0.8784, G Loss: 1.6837\n",
      "Epoch: [94/200], Step: [300/600], D Loss: 0.8226, G Loss: 1.9375\n",
      "Epoch: [94/200], Step: [600/600], D Loss: 0.7358, G Loss: 1.7195\n",
      "Epoch: [95/200], Step: [300/600], D Loss: 0.7703, G Loss: 1.8907\n",
      "Epoch: [95/200], Step: [600/600], D Loss: 0.9709, G Loss: 1.7465\n",
      "Epoch: [96/200], Step: [300/600], D Loss: 0.7684, G Loss: 1.8041\n",
      "Epoch: [96/200], Step: [600/600], D Loss: 0.8331, G Loss: 1.5269\n",
      "Epoch: [97/200], Step: [300/600], D Loss: 1.0717, G Loss: 1.5391\n",
      "Epoch: [97/200], Step: [600/600], D Loss: 0.8650, G Loss: 1.8802\n",
      "Epoch: [98/200], Step: [300/600], D Loss: 0.8608, G Loss: 1.8227\n",
      "Epoch: [98/200], Step: [600/600], D Loss: 0.9829, G Loss: 1.4090\n",
      "Epoch: [99/200], Step: [300/600], D Loss: 0.7506, G Loss: 1.3966\n",
      "Epoch: [99/200], Step: [600/600], D Loss: 0.8483, G Loss: 1.8147\n",
      "Epoch: [100/200], Step: [300/600], D Loss: 0.6879, G Loss: 2.0034\n",
      "Epoch: [100/200], Step: [600/600], D Loss: 0.7719, G Loss: 1.6748\n",
      "Epoch: [101/200], Step: [300/600], D Loss: 0.8560, G Loss: 1.4617\n",
      "Epoch: [101/200], Step: [600/600], D Loss: 0.9423, G Loss: 1.5902\n",
      "Epoch: [102/200], Step: [300/600], D Loss: 0.7736, G Loss: 1.7983\n",
      "Epoch: [102/200], Step: [600/600], D Loss: 0.8462, G Loss: 1.8269\n",
      "Epoch: [103/200], Step: [300/600], D Loss: 0.8013, G Loss: 1.7610\n",
      "Epoch: [103/200], Step: [600/600], D Loss: 0.8316, G Loss: 1.5639\n",
      "Epoch: [104/200], Step: [300/600], D Loss: 0.7791, G Loss: 1.4495\n",
      "Epoch: [104/200], Step: [600/600], D Loss: 0.8430, G Loss: 1.9114\n",
      "Epoch: [105/200], Step: [300/600], D Loss: 0.7594, G Loss: 1.9716\n",
      "Epoch: [105/200], Step: [600/600], D Loss: 0.8235, G Loss: 1.8045\n",
      "Epoch: [106/200], Step: [300/600], D Loss: 0.9264, G Loss: 1.9475\n",
      "Epoch: [106/200], Step: [600/600], D Loss: 0.8518, G Loss: 1.5762\n",
      "Epoch: [107/200], Step: [300/600], D Loss: 0.8488, G Loss: 1.5565\n",
      "Epoch: [107/200], Step: [600/600], D Loss: 0.8304, G Loss: 1.5307\n",
      "Epoch: [108/200], Step: [300/600], D Loss: 0.6965, G Loss: 1.6122\n",
      "Epoch: [108/200], Step: [600/600], D Loss: 0.8939, G Loss: 1.5620\n",
      "Epoch: [109/200], Step: [300/600], D Loss: 0.9929, G Loss: 1.7652\n",
      "Epoch: [109/200], Step: [600/600], D Loss: 0.8597, G Loss: 1.4632\n",
      "Epoch: [110/200], Step: [300/600], D Loss: 0.8287, G Loss: 2.0286\n",
      "Epoch: [110/200], Step: [600/600], D Loss: 0.7456, G Loss: 1.6763\n",
      "Epoch: [111/200], Step: [300/600], D Loss: 0.8237, G Loss: 1.5615\n",
      "Epoch: [111/200], Step: [600/600], D Loss: 0.8552, G Loss: 1.7530\n",
      "Epoch: [112/200], Step: [300/600], D Loss: 0.8397, G Loss: 1.8238\n",
      "Epoch: [112/200], Step: [600/600], D Loss: 0.8585, G Loss: 1.4031\n",
      "Epoch: [113/200], Step: [300/600], D Loss: 0.7890, G Loss: 1.9771\n",
      "Epoch: [113/200], Step: [600/600], D Loss: 0.8130, G Loss: 1.5052\n",
      "Epoch: [114/200], Step: [300/600], D Loss: 0.7754, G Loss: 1.5975\n",
      "Epoch: [114/200], Step: [600/600], D Loss: 0.9052, G Loss: 1.5517\n",
      "Epoch: [115/200], Step: [300/600], D Loss: 1.0110, G Loss: 1.4700\n",
      "Epoch: [115/200], Step: [600/600], D Loss: 0.8345, G Loss: 1.4790\n",
      "Epoch: [116/200], Step: [300/600], D Loss: 0.8098, G Loss: 1.8060\n",
      "Epoch: [116/200], Step: [600/600], D Loss: 0.9889, G Loss: 1.5849\n",
      "Epoch: [117/200], Step: [300/600], D Loss: 0.7775, G Loss: 1.8266\n",
      "Epoch: [117/200], Step: [600/600], D Loss: 0.9670, G Loss: 1.6203\n",
      "Epoch: [118/200], Step: [300/600], D Loss: 0.8271, G Loss: 1.6783\n",
      "Epoch: [118/200], Step: [600/600], D Loss: 0.7392, G Loss: 1.9228\n",
      "Epoch: [119/200], Step: [300/600], D Loss: 1.0539, G Loss: 1.6542\n",
      "Epoch: [119/200], Step: [600/600], D Loss: 0.8279, G Loss: 1.4572\n",
      "Epoch: [120/200], Step: [300/600], D Loss: 0.7519, G Loss: 1.8515\n",
      "Epoch: [120/200], Step: [600/600], D Loss: 0.8241, G Loss: 1.7438\n",
      "Epoch: [121/200], Step: [300/600], D Loss: 0.8776, G Loss: 1.8407\n",
      "Epoch: [121/200], Step: [600/600], D Loss: 0.8693, G Loss: 1.8487\n",
      "Epoch: [122/200], Step: [300/600], D Loss: 0.9158, G Loss: 1.9248\n",
      "Epoch: [122/200], Step: [600/600], D Loss: 0.9537, G Loss: 1.7595\n",
      "Epoch: [123/200], Step: [300/600], D Loss: 0.9469, G Loss: 1.6421\n",
      "Epoch: [123/200], Step: [600/600], D Loss: 0.9238, G Loss: 1.5100\n",
      "Epoch: [124/200], Step: [300/600], D Loss: 0.8184, G Loss: 1.7514\n",
      "Epoch: [124/200], Step: [600/600], D Loss: 0.9200, G Loss: 1.5790\n",
      "Epoch: [125/200], Step: [300/600], D Loss: 0.9547, G Loss: 1.9963\n",
      "Epoch: [125/200], Step: [600/600], D Loss: 0.9183, G Loss: 1.5212\n",
      "Epoch: [126/200], Step: [300/600], D Loss: 0.9056, G Loss: 1.8255\n",
      "Epoch: [126/200], Step: [600/600], D Loss: 0.9549, G Loss: 1.8534\n",
      "Epoch: [127/200], Step: [300/600], D Loss: 0.8751, G Loss: 1.7154\n",
      "Epoch: [127/200], Step: [600/600], D Loss: 0.7934, G Loss: 1.6971\n",
      "Epoch: [128/200], Step: [300/600], D Loss: 0.7400, G Loss: 1.6234\n",
      "Epoch: [128/200], Step: [600/600], D Loss: 0.8629, G Loss: 1.6061\n",
      "Epoch: [129/200], Step: [300/600], D Loss: 0.9931, G Loss: 1.6781\n",
      "Epoch: [129/200], Step: [600/600], D Loss: 0.8180, G Loss: 1.5407\n",
      "Epoch: [130/200], Step: [300/600], D Loss: 0.7609, G Loss: 1.7567\n",
      "Epoch: [130/200], Step: [600/600], D Loss: 0.9404, G Loss: 1.6593\n",
      "Epoch: [131/200], Step: [300/600], D Loss: 0.8866, G Loss: 2.0547\n",
      "Epoch: [131/200], Step: [600/600], D Loss: 0.8879, G Loss: 1.5681\n",
      "Epoch: [132/200], Step: [300/600], D Loss: 0.9740, G Loss: 1.8644\n",
      "Epoch: [132/200], Step: [600/600], D Loss: 0.8355, G Loss: 1.6533\n",
      "Epoch: [133/200], Step: [300/600], D Loss: 0.8362, G Loss: 1.6732\n",
      "Epoch: [133/200], Step: [600/600], D Loss: 1.1069, G Loss: 1.5260\n",
      "Epoch: [134/200], Step: [300/600], D Loss: 0.9873, G Loss: 1.7681\n",
      "Epoch: [134/200], Step: [600/600], D Loss: 0.7864, G Loss: 1.5466\n",
      "Epoch: [135/200], Step: [300/600], D Loss: 0.9159, G Loss: 1.7783\n",
      "Epoch: [135/200], Step: [600/600], D Loss: 0.9256, G Loss: 2.0462\n",
      "Epoch: [136/200], Step: [300/600], D Loss: 0.8559, G Loss: 1.5778\n",
      "Epoch: [136/200], Step: [600/600], D Loss: 0.9114, G Loss: 1.4437\n",
      "Epoch: [137/200], Step: [300/600], D Loss: 0.8342, G Loss: 1.8045\n",
      "Epoch: [137/200], Step: [600/600], D Loss: 0.8892, G Loss: 1.9446\n",
      "Epoch: [138/200], Step: [300/600], D Loss: 0.8100, G Loss: 1.5975\n",
      "Epoch: [138/200], Step: [600/600], D Loss: 0.8661, G Loss: 1.6491\n",
      "Epoch: [139/200], Step: [300/600], D Loss: 0.8228, G Loss: 1.4757\n",
      "Epoch: [139/200], Step: [600/600], D Loss: 0.9028, G Loss: 1.8911\n",
      "Epoch: [140/200], Step: [300/600], D Loss: 0.7695, G Loss: 2.2191\n",
      "Epoch: [140/200], Step: [600/600], D Loss: 0.6880, G Loss: 1.7916\n",
      "Epoch: [141/200], Step: [300/600], D Loss: 0.8899, G Loss: 1.9838\n",
      "Epoch: [141/200], Step: [600/600], D Loss: 0.8940, G Loss: 1.5975\n",
      "Epoch: [142/200], Step: [300/600], D Loss: 0.8094, G Loss: 1.6160\n",
      "Epoch: [142/200], Step: [600/600], D Loss: 0.7510, G Loss: 2.0806\n",
      "Epoch: [143/200], Step: [300/600], D Loss: 0.8780, G Loss: 1.8747\n",
      "Epoch: [143/200], Step: [600/600], D Loss: 0.9499, G Loss: 1.3711\n",
      "Epoch: [144/200], Step: [300/600], D Loss: 0.9464, G Loss: 1.9721\n",
      "Epoch: [144/200], Step: [600/600], D Loss: 0.9723, G Loss: 1.5836\n",
      "Epoch: [145/200], Step: [300/600], D Loss: 0.8047, G Loss: 1.8789\n",
      "Epoch: [145/200], Step: [600/600], D Loss: 0.9168, G Loss: 1.9784\n",
      "Epoch: [146/200], Step: [300/600], D Loss: 0.8326, G Loss: 1.4005\n",
      "Epoch: [146/200], Step: [600/600], D Loss: 0.9973, G Loss: 1.9723\n",
      "Epoch: [147/200], Step: [300/600], D Loss: 0.7697, G Loss: 1.6387\n",
      "Epoch: [147/200], Step: [600/600], D Loss: 0.8657, G Loss: 1.5054\n",
      "Epoch: [148/200], Step: [300/600], D Loss: 0.8704, G Loss: 1.9875\n",
      "Epoch: [148/200], Step: [600/600], D Loss: 0.8968, G Loss: 1.5346\n",
      "Epoch: [149/200], Step: [300/600], D Loss: 0.8826, G Loss: 1.9430\n",
      "Epoch: [149/200], Step: [600/600], D Loss: 0.8396, G Loss: 1.6668\n",
      "Epoch: [150/200], Step: [300/600], D Loss: 0.8050, G Loss: 1.6722\n",
      "Epoch: [150/200], Step: [600/600], D Loss: 0.8527, G Loss: 1.6869\n",
      "Epoch: [151/200], Step: [300/600], D Loss: 1.0027, G Loss: 2.3121\n",
      "Epoch: [151/200], Step: [600/600], D Loss: 0.8486, G Loss: 1.7538\n",
      "Epoch: [152/200], Step: [300/600], D Loss: 0.7857, G Loss: 1.6489\n",
      "Epoch: [152/200], Step: [600/600], D Loss: 0.9072, G Loss: 1.4486\n",
      "Epoch: [153/200], Step: [300/600], D Loss: 0.9372, G Loss: 1.6228\n",
      "Epoch: [153/200], Step: [600/600], D Loss: 0.9003, G Loss: 1.6465\n",
      "Epoch: [154/200], Step: [300/600], D Loss: 0.8456, G Loss: 1.9322\n",
      "Epoch: [154/200], Step: [600/600], D Loss: 0.8591, G Loss: 1.5929\n",
      "Epoch: [155/200], Step: [300/600], D Loss: 0.8470, G Loss: 1.6647\n",
      "Epoch: [155/200], Step: [600/600], D Loss: 0.8310, G Loss: 1.6682\n",
      "Epoch: [156/200], Step: [300/600], D Loss: 0.7971, G Loss: 1.8430\n",
      "Epoch: [156/200], Step: [600/600], D Loss: 0.7160, G Loss: 1.7268\n",
      "Epoch: [157/200], Step: [300/600], D Loss: 0.8246, G Loss: 1.6967\n",
      "Epoch: [157/200], Step: [600/600], D Loss: 0.8057, G Loss: 1.6236\n",
      "Epoch: [158/200], Step: [300/600], D Loss: 0.8194, G Loss: 1.4223\n",
      "Epoch: [158/200], Step: [600/600], D Loss: 0.8079, G Loss: 1.8342\n",
      "Epoch: [159/200], Step: [300/600], D Loss: 1.0205, G Loss: 1.5792\n",
      "Epoch: [159/200], Step: [600/600], D Loss: 0.8948, G Loss: 1.6119\n",
      "Epoch: [160/200], Step: [300/600], D Loss: 0.9500, G Loss: 1.3596\n",
      "Epoch: [160/200], Step: [600/600], D Loss: 0.8706, G Loss: 1.7757\n",
      "Epoch: [161/200], Step: [300/600], D Loss: 0.8234, G Loss: 1.8421\n",
      "Epoch: [161/200], Step: [600/600], D Loss: 0.8208, G Loss: 1.6176\n",
      "Epoch: [162/200], Step: [300/600], D Loss: 0.6921, G Loss: 1.3659\n",
      "Epoch: [162/200], Step: [600/600], D Loss: 0.8807, G Loss: 1.8143\n",
      "Epoch: [163/200], Step: [300/600], D Loss: 1.1308, G Loss: 2.1176\n",
      "Epoch: [163/200], Step: [600/600], D Loss: 0.8507, G Loss: 1.8048\n",
      "Epoch: [164/200], Step: [300/600], D Loss: 0.7850, G Loss: 1.7954\n",
      "Epoch: [164/200], Step: [600/600], D Loss: 0.9295, G Loss: 2.0483\n",
      "Epoch: [165/200], Step: [300/600], D Loss: 0.8487, G Loss: 1.7619\n",
      "Epoch: [165/200], Step: [600/600], D Loss: 0.8452, G Loss: 1.7520\n",
      "Epoch: [166/200], Step: [300/600], D Loss: 0.9078, G Loss: 1.8639\n",
      "Epoch: [166/200], Step: [600/600], D Loss: 0.7477, G Loss: 1.6698\n",
      "Epoch: [167/200], Step: [300/600], D Loss: 0.9844, G Loss: 1.5652\n",
      "Epoch: [167/200], Step: [600/600], D Loss: 0.7913, G Loss: 1.8668\n",
      "Epoch: [168/200], Step: [300/600], D Loss: 0.8907, G Loss: 1.5165\n",
      "Epoch: [168/200], Step: [600/600], D Loss: 1.0468, G Loss: 1.7282\n",
      "Epoch: [169/200], Step: [300/600], D Loss: 0.8098, G Loss: 1.1101\n",
      "Epoch: [169/200], Step: [600/600], D Loss: 0.8768, G Loss: 1.6335\n",
      "Epoch: [170/200], Step: [300/600], D Loss: 0.9002, G Loss: 1.6833\n",
      "Epoch: [170/200], Step: [600/600], D Loss: 0.8604, G Loss: 1.7760\n",
      "Epoch: [171/200], Step: [300/600], D Loss: 0.8980, G Loss: 1.5162\n",
      "Epoch: [171/200], Step: [600/600], D Loss: 0.8024, G Loss: 1.5447\n",
      "Epoch: [172/200], Step: [300/600], D Loss: 0.7977, G Loss: 1.8197\n",
      "Epoch: [172/200], Step: [600/600], D Loss: 0.8630, G Loss: 1.7789\n",
      "Epoch: [173/200], Step: [300/600], D Loss: 0.8010, G Loss: 2.1501\n",
      "Epoch: [173/200], Step: [600/600], D Loss: 0.9318, G Loss: 1.4830\n",
      "Epoch: [174/200], Step: [300/600], D Loss: 0.8601, G Loss: 1.2659\n",
      "Epoch: [174/200], Step: [600/600], D Loss: 0.7850, G Loss: 1.8318\n",
      "Epoch: [175/200], Step: [300/600], D Loss: 0.9661, G Loss: 1.6265\n",
      "Epoch: [175/200], Step: [600/600], D Loss: 0.8595, G Loss: 1.6917\n",
      "Epoch: [176/200], Step: [300/600], D Loss: 0.8214, G Loss: 2.0041\n",
      "Epoch: [176/200], Step: [600/600], D Loss: 0.8449, G Loss: 2.1012\n",
      "Epoch: [177/200], Step: [300/600], D Loss: 0.8409, G Loss: 1.8197\n",
      "Epoch: [177/200], Step: [600/600], D Loss: 1.0704, G Loss: 1.3132\n",
      "Epoch: [178/200], Step: [300/600], D Loss: 0.7502, G Loss: 1.7763\n",
      "Epoch: [178/200], Step: [600/600], D Loss: 0.8411, G Loss: 1.8244\n",
      "Epoch: [179/200], Step: [300/600], D Loss: 0.8019, G Loss: 1.4363\n",
      "Epoch: [179/200], Step: [600/600], D Loss: 0.8390, G Loss: 1.5996\n",
      "Epoch: [180/200], Step: [300/600], D Loss: 0.8826, G Loss: 1.9026\n",
      "Epoch: [180/200], Step: [600/600], D Loss: 0.7619, G Loss: 1.8634\n",
      "Epoch: [181/200], Step: [300/600], D Loss: 0.8303, G Loss: 2.0685\n",
      "Epoch: [181/200], Step: [600/600], D Loss: 0.7939, G Loss: 1.7221\n",
      "Epoch: [182/200], Step: [300/600], D Loss: 0.8915, G Loss: 1.9685\n",
      "Epoch: [182/200], Step: [600/600], D Loss: 0.9127, G Loss: 1.7889\n",
      "Epoch: [183/200], Step: [300/600], D Loss: 0.6376, G Loss: 2.1181\n",
      "Epoch: [183/200], Step: [600/600], D Loss: 0.9457, G Loss: 1.7529\n",
      "Epoch: [184/200], Step: [300/600], D Loss: 0.6852, G Loss: 2.0223\n",
      "Epoch: [184/200], Step: [600/600], D Loss: 0.8324, G Loss: 1.7965\n",
      "Epoch: [185/200], Step: [300/600], D Loss: 0.9158, G Loss: 1.7418\n",
      "Epoch: [185/200], Step: [600/600], D Loss: 0.8017, G Loss: 1.6529\n",
      "Epoch: [186/200], Step: [300/600], D Loss: 0.8524, G Loss: 1.7782\n",
      "Epoch: [186/200], Step: [600/600], D Loss: 0.9623, G Loss: 1.9698\n",
      "Epoch: [187/200], Step: [300/600], D Loss: 0.8029, G Loss: 1.9454\n",
      "Epoch: [187/200], Step: [600/600], D Loss: 1.0794, G Loss: 1.7446\n",
      "Epoch: [188/200], Step: [300/600], D Loss: 0.8297, G Loss: 1.8916\n",
      "Epoch: [188/200], Step: [600/600], D Loss: 0.8286, G Loss: 1.8607\n",
      "Epoch: [189/200], Step: [300/600], D Loss: 0.9076, G Loss: 1.9114\n",
      "Epoch: [189/200], Step: [600/600], D Loss: 0.8563, G Loss: 1.9986\n",
      "Epoch: [190/200], Step: [300/600], D Loss: 0.9315, G Loss: 1.8339\n",
      "Epoch: [190/200], Step: [600/600], D Loss: 0.7872, G Loss: 1.6350\n",
      "Epoch: [191/200], Step: [300/600], D Loss: 0.8574, G Loss: 1.6844\n",
      "Epoch: [191/200], Step: [600/600], D Loss: 0.6883, G Loss: 1.4185\n",
      "Epoch: [192/200], Step: [300/600], D Loss: 0.9236, G Loss: 1.8464\n",
      "Epoch: [192/200], Step: [600/600], D Loss: 0.6799, G Loss: 1.3890\n",
      "Epoch: [193/200], Step: [300/600], D Loss: 0.8494, G Loss: 1.6323\n",
      "Epoch: [193/200], Step: [600/600], D Loss: 0.9226, G Loss: 1.6918\n",
      "Epoch: [194/200], Step: [300/600], D Loss: 0.8172, G Loss: 2.0074\n",
      "Epoch: [194/200], Step: [600/600], D Loss: 0.9109, G Loss: 1.6608\n",
      "Epoch: [195/200], Step: [300/600], D Loss: 0.7892, G Loss: 2.2535\n",
      "Epoch: [195/200], Step: [600/600], D Loss: 0.8280, G Loss: 2.2007\n",
      "Epoch: [196/200], Step: [300/600], D Loss: 0.8066, G Loss: 2.1888\n",
      "Epoch: [196/200], Step: [600/600], D Loss: 1.0189, G Loss: 1.5557\n",
      "Epoch: [197/200], Step: [300/600], D Loss: 0.7739, G Loss: 1.4721\n",
      "Epoch: [197/200], Step: [600/600], D Loss: 0.9256, G Loss: 1.1746\n",
      "Epoch: [198/200], Step: [300/600], D Loss: 0.7818, G Loss: 1.9561\n",
      "Epoch: [198/200], Step: [600/600], D Loss: 0.8911, G Loss: 2.1593\n",
      "Epoch: [199/200], Step: [300/600], D Loss: 0.6944, G Loss: 1.9513\n",
      "Epoch: [199/200], Step: [600/600], D Loss: 0.8504, G Loss: 1.8651\n",
      "Epoch: [200/200], Step: [300/600], D Loss: 0.8555, G Loss: 1.9376\n",
      "Epoch: [200/200], Step: [600/600], D Loss: 0.9182, G Loss: 1.6849\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    for i, (inputs, _) in enumerate(train_loader):\n",
    "        \n",
    "        # Generator 학습\n",
    "        real_img = Variable(inputs.view(-1,28*28))\n",
    "        real_label = Variable(torch.ones(inputs.size(0)))\n",
    "        real_preds = Discriminator(real_img)\n",
    "        \n",
    "        latent = Variable(torch.randn(inputs.size(0),LATENT_SIZE))\n",
    "        fake_img = Generator(latent)\n",
    "        fake_label = Variable(torch.zeros(inputs.size(0)))\n",
    "        fake_preds = Discriminator(fake_img)\n",
    "        \n",
    "        d_loss_1 = loss_function(real_preds.squeeze(1),real_label)\n",
    "        d_loss_2 = loss_function(fake_preds.squeeze(1),fake_label)\n",
    "        \n",
    "        d_loss = d_loss_1 + d_loss_2\n",
    "        \n",
    "        Discriminator.zero_grad()\n",
    "        Generator.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        # Discriminator 학습\n",
    "        latent = Variable(torch.randn(inputs.size(0),LATENT_SIZE))\n",
    "        fake_img = Generator(latent)\n",
    "        preds = Discriminator(fake_img)\n",
    "        \n",
    "        g_loss = loss_function(preds.squeeze(1),real_label)\n",
    "        \n",
    "        Discriminator.zero_grad()\n",
    "        Generator.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        if (i+1) % 300 == 0:\n",
    "            print ('Epoch: [%d/%d], Step: [%d/%d], D Loss: %.4f, G Loss: %.4f' \n",
    "                   % (epoch+1, EPOCH, i+1, len(train_dataset)//BATCH_SIZE, d_loss.data[0],g_loss.data[0]))\n",
    "            \n",
    "    # 생성한 이미지 샘플 저장\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        fake_img = fake_img.view(fake_img.size(0), 1, 28, 28)\n",
    "        save_image(denorm(fake_img.data), './images/gan_gen_images-%d.png' %(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "latent = Variable(torch.randn(1,LATENT_SIZE),volatile=True)\n",
    "fake_img = Generator(latent)\n",
    "\n",
    "plt.matshow(np.reshape(denorm(fake_img).data.numpy(), (28, 28)), cmap=plt.get_cmap('gray'))\n",
    "plt.title(\"[\" + str(epoch) + \"] Generated Image\\n\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
