{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from konlpy.tag import Kkma\n",
    "tagger= Kkma()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RNNCell "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h′=tanh(wih∗x+bih+whh∗h+bhh)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size = 10 # input dimension (word embedding) D\n",
    "hidden_size = 30 # hidden dimension H\n",
    "batch_size = 3\n",
    "length = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnncell = nn.RNNCell(input_size=input_size,hidden_size=hidden_size,bias=True,nonlinearity='tanh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNCell(10, 30)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnncell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input = Variable(torch.randn(length, batch_size, input_size)) # T, B, D    (embedding matrix에서 인덱싱한 워드 벡터)\n",
    "hidden = Variable(torch.zeros(batch_size, hidden_size)) # first hidden state\n",
    "output = []\n",
    "for i in range(length):\n",
    "    hidden = rnncell(input[i], hidden)\n",
    "    output.append(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 30])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.size() # Batch size, Hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size = 10\n",
    "hidden_size = 30\n",
    "batch_size = 3\n",
    "length = 4\n",
    "output_size = 5\n",
    "\n",
    "rnn = nn.RNN(input_size, hidden_size,batch_first=True) #,num_layers=1,bias=True,nonlinearity='tanh', batch_first=True, dropout=0, bidirectional=False)\n",
    "\n",
    "# (num_layers * num_directions, batch, hidden_size)\n",
    "input = Variable(torch.randn(batch_size,length,input_size)) # B,T,D  <= batch_first\n",
    "hidden = Variable(torch.zeros(1,batch_size,hidden_size)) # 1,B,H    (num_layers * num_directions, batch, hidden_size)\n",
    "\n",
    "output, hidden = rnn(input,hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 30])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size() # B,T,H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 30])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.size() # 1,B,H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 5])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = nn.Linear(hidden_size,output_size)\n",
    "output = F.softmax(linear(output),1)\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bidirectional RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnn = nn.RNN(input_size, hidden_size,num_layers=1,bias=True,nonlinearity='tanh', batch_first=True, dropout=0, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input = Variable(torch.randn(batch_size,length,input_size)) # B,T,D\n",
    "hidden = Variable(torch.zeros(2,batch_size,hidden_size)) # 2,B,H    (num_layers * num_directions, batch, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output, hidden = rnn(input,hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 60])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size() # concat of forward,backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 30])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.size() # forward, backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-layer RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn = nn.RNN(input_size, hidden_size, num_layers=3,bias=True, nonlinearity='tanh', batch_first=True, dropout=0, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input = Variable(torch.randn(batch_size,length,input_size)) # B,T,D\n",
    "hidden = Variable(torch.zeros(3*2,batch_size,hidden_size)) # 6,B,H    (num_layers * num_directions, batch, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output, hidden = rnn(input,hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 60])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3, 30])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.size() # (forward, backward)*num_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. GRU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnn = nn.GRU(input_size,hidden_size,batch_first=True) #,num_layers=1,bias=True,batch_first=True,bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input = Variable(torch.randn(batch_size,length,input_size)) # B,T,D\n",
    "hidden = Variable(torch.zeros(1,batch_size,hidden_size)) # 2,B,H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output, hidden = rnn(input,hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 30])\n",
      "torch.Size([1, 3, 30])\n"
     ]
    }
   ],
   "source": [
    "print(output.size())\n",
    "print(hidden.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size = 10\n",
    "hidden_size = 30\n",
    "output_size = 10\n",
    "batch_size = 3\n",
    "length = 4\n",
    "num_layers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnn = nn.LSTM(input_size,hidden_size,batch_first=True) #,num_layers=num_layers,bias=True,batch_first=True,bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input = Variable(torch.randn(batch_size,length,input_size)) # B,T,D\n",
    "hidden = Variable(torch.zeros(1,batch_size,hidden_size)) # (num_layers * num_directions, batch, hidden_size)\n",
    "cell = Variable(torch.zeros(1,batch_size,hidden_size)) # (num_layers * num_directions, batch, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input = Variable(torch.randn(batch_size,length,input_size)) # B,T,D\n",
    "hidden = Variable(torch.zeros(num_layers*2,batch_size,hidden_size)) # (num_layers * num_directions, batch, hidden_size)\n",
    "cell = Variable(torch.zeros(num_layers*2,batch_size,hidden_size)) # (num_layers * num_directions, batch, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output, (hidden,cell) = rnn(input,(hidden,cell))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 30])\n",
      "torch.Size([1, 3, 30])\n",
      "torch.Size([1, 3, 30])\n"
     ]
    }
   ],
   "source": [
    "print(output.size())\n",
    "print(hidden.size())\n",
    "print(cell.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 10])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = nn.Linear(hidden_size*2,output_size)\n",
    "output = F.softmax(linear(output),1)\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 각 timestep마다 그 다음에 올 단어를 예측하는 Language model을 만드시오 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 다음 Corpus(sentences)를 tokenized하고 Vocab을 만드시오\n",
    "* Embedding matrix(vector size는 10)\n",
    "* hidden state의 size가 20인 Bidirectional GRU(num_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma\n",
    "tagger = Kkma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences=[\"나는 오늘 삼계탕을 먹었다\",\"그런데도 배가 아직 고프다\",\"이제 영화보러 가야겠다 요즘 뭐가 재밌지\",\"날씨가 진짜 좋다\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized = [tagger.morphs(s) for s in sentences]\n",
    "vocab = list((set([token for tokens in tokenized for token in tokens])))\n",
    "word2index = {v:i for i,v in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "V = len(word2index)\n",
    "D = 10\n",
    "H = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LM(nn.Module):\n",
    "    def __init__(self,V,D,H):\n",
    "        super(LM,self).__init__()\n",
    "        self.hidden_size = H\n",
    "        \n",
    "        self.embed = nn.Embedding(V,D) # VxD\n",
    "        self.gru = nn.GRU(D,H,1,batch_first=True,bidirectional=True)\n",
    "        self.linear = nn.Linear(H*2,V)\n",
    "    \n",
    "    def init_hidden(self,batch_size):\n",
    "        hidden = Variable(torch.zeros(2,batch_size,self.hidden_size))\n",
    "        return hidden\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        \"\"\"\n",
    "        inputs : B,T # LongTensor\n",
    "        \"\"\"\n",
    "        embed = self.embed(inputs) # B,T,D\n",
    "        hidden = self.init_hidden(inputs.size(0)) # 2,B,H\n",
    "        output, hidden = self.gru(embed,hidden)\n",
    "        # output : B,T,2H\n",
    "        # hidden : 2,B,H\n",
    "        \n",
    "        output = self.linear(output) # B,T,V\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LM(len(word2index),10,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Standard form "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self,input_size,embed_size,hidden_size,output_size,num_layers=1,bidirec=False):\n",
    "        super(RNN,self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        if bidirec:\n",
    "            self.num_directions = 2\n",
    "        else:\n",
    "            self.num_directions = 1\n",
    "            \n",
    "        self.embed = nn.Embedding(input_size,embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size,hidden_size,num_layers,batch_first=True,bidirectional=bidirec)\n",
    "        self.linear = nn.Linear(hidden_size*self.num_directions,output_size)\n",
    "        \n",
    "    def init_hidden(self,batch_size):\n",
    "        # (num_layers * num_directions, batch_size, hidden_size)\n",
    "        hidden = Variable(torch.zeros(self.num_layers*self.num_directions,batch_size,self.hidden_size))\n",
    "        cell = Variable(torch.zeros(self.num_layers*self.num_directions,batch_size,self.hidden_size))\n",
    "        return hidden, cell\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        \"\"\"\n",
    "        inputs : B,T\n",
    "        \"\"\"\n",
    "        embed = self.embed(inputs) # word vector indexing\n",
    "        hidden, cell = self.init_hidden(inputs.size(0)) # initial hidden,cell\n",
    "        \n",
    "        output, (hidden,cell) = self.lstm(embed,(hidden,cell))\n",
    "        \n",
    "        # Many-to-Many\n",
    "        output = self.linear(output) # B,T,H -> B,T,V\n",
    "        \n",
    "        # Many-to-One\n",
    "        #hidden = hidden[-self.num_directions:] # (num_directions,B,H)\n",
    "        #hidden = torch.cat([h for h in hidden],1)\n",
    "        #output = self.linear(hidden) # last hidden\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB=1000 # input_size\n",
    "EMBED = 50 # embedding_size\n",
    "HIDDEN = 100 # hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn = RNN(VOCAB,EMBED,HIDDEN,VOCAB,bidirec=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_input = Variable(torch.randperm(32*10)).view(32,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "    7   231   163   119   173   222    66    83    25   176\n",
       "   28    64   156    87   268   153    62    86   177   224\n",
       "  317   241   306    78     2   149    38   110   309   300\n",
       "  155   273   207   205   150    88    16     5   191   108\n",
       "   54   127   154    10   303    71   129   264   187    61\n",
       "  230    46   151   122    65   135   284    98   260   157\n",
       "  240    60    30   107   184   232   142   138    26   130\n",
       "  213   302   277    14   275   310   140   238   243    96\n",
       "  283    99   218   115   144    59   199   196   145   248\n",
       "  313   291   166   295   114    39   186   276    57   132\n",
       "  111   270    27   152   280   247   200    42   192   252\n",
       "  272   269   147   168   106   126   162    81   208   282\n",
       "  312    44    37   182    13   288   209   263    31   258\n",
       "  318   113   297    95   188   120   244   274   116   158\n",
       "  164   229   305   220    91   143    84    12    67   228\n",
       "  100    47   316   216   249   204    68    69   206    53\n",
       "    8    48    29   314   262   239   253   250    41   259\n",
       "  221   311    93   137   105   211   121   104   223   292\n",
       "   23    51   181   281   315    50    63   215   180   161\n",
       "  189    90   246   255    22   212   267   307    18   109\n",
       "   72   278   237   175     0    85    20    35   308   234\n",
       "  296   289    55     3    74    76   171    11   169    73\n",
       "  256   195   261   146   185   148   233   287    82   219\n",
       "  174   197    17    70   123   179    80   290   136   118\n",
       "  226    34   271   217   134   102   131     6   293   210\n",
       "   43    45   170    58   103   266   304     4   178    79\n",
       "  101    89   172   225   117    56   165   128    36   193\n",
       "   52    92   236   227   299   190   286    97   183   124\n",
       "  214   202   112   294   201    75    32    77    15    49\n",
       "  125   133   203    40   265    94    19   251   141    21\n",
       "    9   139   242   235   279    24   160   198   298   257\n",
       "  254   167   194   301     1   159    33   245   319   285\n",
       "[torch.LongTensor of size 32x10]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input # 길이 10개짜리 문장의 32개 배치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = rnn(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 1000])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## TODO : Sentence Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* data/train.txt를 torchtext로 load하시오(배치사이즈 5)\n",
    "* Bidirectional LSTM을 선언(num_layers=2)\n",
    "* 마지막 히든 스테이트를 이용하여 Binary Classifier를 만드시오(Many-to-One)\n",
    "* train 시키시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchtext.data import Field,Iterator,TabularDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TEXT = Field(tokenize=tagger.morphs,use_vocab=True, batch_first=True)\n",
    "LABEL = Field(sequential=False,use_vocab=True,unk_token=None)\n",
    "\n",
    "train_data = TabularDataset(path=\"data/train.txt\",\n",
    "                                          format=\"tsv\",\n",
    "                                          fields=[('TEXT',TEXT),('LABEL',LABEL)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['배고프', '다', '밥', '주', '어']\n",
      "FOOD\n"
     ]
    }
   ],
   "source": [
    "print(train_data.examples[0].TEXT)\n",
    "print(train_data.examples[0].LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader =  Iterator(train_data, batch_size=5, device=-1, # device -1 : cpu, device 0 : 남는 gpu\n",
    "    sort_key=lambda x: len(x.TEXT),sort_within_batch=True,repeat=False) # x.TEXT 길이 기준으로 정렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self,input_size,embed_size,hidden_size,output_size,num_layers=1,bidirec=False):\n",
    "        super(RNN,self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        if bidirec:\n",
    "            self.num_directions = 2\n",
    "        else:\n",
    "            self.num_directions = 1\n",
    "            \n",
    "        self.embed = nn.Embedding(input_size,embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size,hidden_size,num_layers,batch_first=True,bidirectional=bidirec)\n",
    "        self.linear = nn.Linear(hidden_size*self.num_directions,output_size)\n",
    "        \n",
    "    def init_hidden(self,batch_size):\n",
    "        # (num_layers * num_directions, batch_size, hidden_size)\n",
    "        hidden = Variable(torch.zeros(self.num_layers*self.num_directions,batch_size,self.hidden_size))\n",
    "        cell = Variable(torch.zeros(self.num_layers*self.num_directions,batch_size,self.hidden_size))\n",
    "        return hidden, cell\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        \"\"\"\n",
    "        inputs : B,T\n",
    "        \"\"\"\n",
    "        embed = self.embed(inputs) # word vector indexing\n",
    "        hidden, cell = self.init_hidden(inputs.size(0)) # initial hidden,cell\n",
    "        \n",
    "        output, (hidden,cell) = self.lstm(embed,(hidden,cell))\n",
    "        \n",
    "        # Many-to-One\n",
    "        hidden = hidden[-self.num_directions:] # (num_directions,B,H)\n",
    "        hidden = torch.cat([h for h in hidden],1)\n",
    "        output = self.linear(hidden) # last hidden\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = RNN(len(TEXT.vocab),30,50,len(LABEL.vocab),bidirec=True)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16062602400779724\n",
      "0.07795029878616333\n",
      "0.10190250724554062\n",
      "0.16149498522281647\n",
      "0.23214185237884521\n",
      "0.10463999211788177\n",
      "0.14870978891849518\n",
      "0.0772019475698471\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    model.zero_grad()\n",
    "    preds = model(batch.TEXT)\n",
    "    loss = loss_function(preds,batch.LABEL)\n",
    "    print(loss.data[0])\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
