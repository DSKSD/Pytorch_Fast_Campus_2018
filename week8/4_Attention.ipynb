{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "B = 1  # 배치 사이즈\n",
    "E = 30 # 워드임베딩 사이즈\n",
    "T = 5 # 인풋 문장 길이(임의로 지정)\n",
    "H = 50 # 히든 스테이트 사이즈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = Variable(torch.randn(B,T,E))\n",
    "hidden = Variable(torch.zeros(1,B,H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gru = nn.GRU(E,H,batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_hiddens,hidden = gru(inputs,hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 50])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_hiddens.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_hidden = Variable(torch.randn(1,B,H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 50])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_hidden.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일단 배치를 생략하고 구해본다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 50]) torch.Size([1, 50])\n"
     ]
    }
   ],
   "source": [
    "encoder_hiddens = encoder_hiddens.squeeze(0) # B 제거\n",
    "decoder_hidden = decoder_hidden.squeeze(1)\n",
    "\n",
    "print(encoder_hiddens.size(),decoder_hidden.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. dot product "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$e_{ti} = s_t^Th_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores=[]\n",
    "for i in range(encoder_hiddens.size(0)):\n",
    "    score = encoder_hiddens[i].dot(decoder_hidden[0])\n",
    "    scores.append(score)\n",
    "    \n",
    "scores = torch.cat(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-3.0896\n",
       "-2.7235\n",
       "-1.1030\n",
       "-0.2602\n",
       " 0.2127\n",
       "[torch.FloatTensor of size 5]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores # attention scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-3.0896\n",
       "-2.7235\n",
       "-1.1030\n",
       "-0.2602\n",
       " 0.2127\n",
       "[torch.FloatTensor of size 5x1]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = encoder_hiddens.matmul(decoder_hidden.transpose(0,1)) # 행렬 연산으로도 가능\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\alpha_{ti}^e=\\frac{exp(e_{ti})}{\\sum_{j=1}^n exp(e_{tj})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "attn_dist = F.softmax(scores,0)\n",
    "print(attn_dist.sum()) # 합이 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$c_t^e = \\sum_i^n \\alpha_{ti}^eh_i^e$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50])\n"
     ]
    }
   ],
   "source": [
    "context_vector = torch.matmul(scores.transpose(0,1),encoder_hiddens) # 행렬곱으로 처리\n",
    "print(context_vector.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO : General format의 Attention 짜보기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $e_{ti} = s_t^TW_{attn}^eh_i$ # attention score\n",
    "2. $\\alpha_{ti}^e=\\frac{exp(e_{ti})}{\\sum_{j=1}^n exp(e_{tj})}$ # attention distribution\n",
    "3. $c_t^e = \\sum_i^n \\alpha_{ti}^eh_i^e$ # context vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_hiddens = Variable(torch.randn(5,50))\n",
    "decoder_hidden = Variable(torch.randn(1,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from attention import Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "B = 32\n",
    "T = 10\n",
    "H = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attn = Attention(50,method='general') # hidden size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_hiddens = Variable(torch.randn(B,T,H))\n",
    "decoder_hidden = Variable(torch.randn(B,1,H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "context_vector = attn(decoder_hidden,encoder_hiddens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 50])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
