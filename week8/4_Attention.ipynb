{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "B = 1  # 배치 사이즈\n",
    "E = 30 # 워드임베딩 사이즈\n",
    "T = 5 # 인풋 문장 길이(임의로 지정)\n",
    "H = 50 # 히든 스테이트 사이즈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = Variable(torch.randn(B,T,E))\n",
    "hidden = Variable(torch.zeros(1,B,H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gru = nn.GRU(E,H,batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_hiddens,hidden = gru(inputs,hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 50])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_hiddens.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_hidden = Variable(torch.randn(1,B,H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 50])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_hidden.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일단 배치를 생략하고 구해본다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 50]) torch.Size([1, 50])\n"
     ]
    }
   ],
   "source": [
    "encoder_hiddens = encoder_hiddens.squeeze(0) # B 제거\n",
    "decoder_hidden = decoder_hidden.squeeze(1)\n",
    "\n",
    "print(encoder_hiddens.size(),decoder_hidden.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. dot product "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$e_{ti} = s_t^Th_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores=[]\n",
    "for i in range(encoder_hiddens.size(0)): # 5번\n",
    "    score = encoder_hiddens[i].dot(decoder_hidden[0])\n",
    "    scores.append(score)\n",
    "    \n",
    "scores = torch.cat(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.4662\n",
       "-0.7955\n",
       "-0.9034\n",
       "-1.2781\n",
       "-1.5351\n",
       "[torch.FloatTensor of size 5]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores # attention scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.4662\n",
       "-0.7955\n",
       "-0.9034\n",
       "-1.2781\n",
       "-1.5351\n",
       "[torch.FloatTensor of size 5x1]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = encoder_hiddens.matmul(decoder_hidden.transpose(0,1)) # 행렬 연산으로도 가능\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\alpha_{ti}^e=\\frac{exp(e_{ti})}{\\sum_{j=1}^n exp(e_{tj})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1.0000\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "attn_dist = F.softmax(scores,0)\n",
    "print(attn_dist.sum()) # 합이 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$c_t^e = \\sum_i^n \\alpha_{ti}^eh_i^e$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50])\n"
     ]
    }
   ],
   "source": [
    "context_vector = torch.matmul(attn_dist.transpose(0,1),encoder_hiddens) # 행렬곱으로 처리\n",
    "print(context_vector.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "\n",
       "Columns 0 to 9 \n",
       " 0.1947 -0.0650  0.0412 -0.1787 -0.0420 -0.1517  0.2607  0.1308 -0.0690  0.0469\n",
       "\n",
       "Columns 10 to 19 \n",
       " 0.1727 -0.1003 -0.2290  0.2873  0.0191 -0.0743  0.0031  0.1621 -0.1171  0.2695\n",
       "\n",
       "Columns 20 to 29 \n",
       "-0.1095 -0.0033  0.2724  0.3487 -0.0757  0.2967  0.2220  0.2021 -0.0911 -0.2859\n",
       "\n",
       "Columns 30 to 39 \n",
       "-0.0298  0.1936  0.1735  0.0958  0.0696 -0.2303 -0.0988  0.1283 -0.1965  0.1781\n",
       "\n",
       "Columns 40 to 49 \n",
       "-0.2700  0.0346 -0.4003  0.2329  0.2719 -0.1430  0.0664  0.1250 -0.1281  0.2943\n",
       "[torch.FloatTensor of size 1x50]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO : General format의 Attention 짜보기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $e_{ti} = s_t^TW_{attn}^eh_i$ # attention score\n",
    "2. $\\alpha_{ti}^e=\\frac{exp(e_{ti})}{\\sum_{j=1}^n exp(e_{tj})}$ # attention distribution\n",
    "3. $c_t^e = \\sum_i^n \\alpha_{ti}^eh_i^e$ # context vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_hiddens = Variable(torch.randn(5,50))\n",
    "decoder_hidden = Variable(torch.randn(1,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight = nn.Parameter(torch.randn(50,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp = torch.mm(decoder_hidden,weight)\n",
    "score = torch.mm(temp,encoder_hiddens.transpose(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "attn_dist = F.softmax(score,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context = torch.mm(attn_dist,encoder_hiddens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "\n",
       "Columns 0 to 9 \n",
       " 0.2467  0.0076  0.1370 -0.1971 -0.0970 -0.1184  0.3326  0.2212 -0.0090  0.0905\n",
       "\n",
       "Columns 10 to 19 \n",
       " 0.1392 -0.0486 -0.2314  0.2763 -0.0699  0.0065  0.0566  0.2061 -0.1480  0.2958\n",
       "\n",
       "Columns 20 to 29 \n",
       "-0.1526 -0.1308  0.3247  0.3574 -0.1501  0.2121  0.1747  0.1161 -0.1306 -0.1848\n",
       "\n",
       "Columns 30 to 39 \n",
       "-0.0611  0.1190  0.0670  0.1577  0.0643 -0.2923 -0.1487  0.0171 -0.2116  0.1260\n",
       "\n",
       "Columns 40 to 49 \n",
       "-0.3304  0.1175 -0.4394  0.1447  0.2981 -0.2352  0.2627  0.1207 -0.1804  0.2333\n",
       "[torch.FloatTensor of size 1x50]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from attention import Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "B = 32\n",
    "T = 10\n",
    "H = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attn = Attention(50,method='general') # hidden size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_hiddens = Variable(torch.randn(B,T,H))\n",
    "decoder_hidden = Variable(torch.randn(B,1,H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "context_vector = attn(decoder_hidden,encoder_hiddens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 50])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
