{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchtext\n",
    "import nltk\n",
    "from konlpy.tag import Kkma\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from attention import Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = 0 if USE_CUDA else -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 준비 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kor_tagger = Kkma()\n",
    "\n",
    "kor_tagger = kor_tagger.morphs\n",
    "eng_tagger = nltk.word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SOURCE = Field(tokenize=kor_tagger,use_vocab=True,init_token=\"<s>\",eos_token=\"</s>\",lower=True, include_lengths=True, batch_first=True)\n",
    "TARGET = Field(tokenize=eng_tagger,use_vocab=True,init_token=\"<s>\",eos_token=\"</s>\",lower=True, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = TabularDataset(\n",
    "                                   path=\"data/parallel_data.txt\",\n",
    "                                   format='tsv', # \\t로 구분\n",
    "                                   #skip_header=True, # 헤더가 있다면 스킵\n",
    "                                   fields=[('inputs',SOURCE),('targets',TARGET)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SOURCE.build_vocab(train_data)\n",
    "TARGET.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1307 1149\n"
     ]
    }
   ],
   "source": [
    "print(len(SOURCE.vocab),len(TARGET.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = BucketIterator(\n",
    "    train_data, batch_size=32, device=DEVICE, # device -1 : cpu, device 0 : 남는 gpu\n",
    "    sort_key=lambda x: len(x.inputs),sort_within_batch=True,repeat=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,V,E,H,num_layers=1,bidirec=False):\n",
    "        super(Encoder,self).__init__()\n",
    "        \n",
    "        self.num_directions = 2 if bidirec else 1\n",
    "        self.embed = nn.Embedding(V,E)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.gru = nn.GRU(E,H,num_layers,batch_first=True,bidirectional=bidirec)\n",
    "            \n",
    "    def forward(self,inputs,input_lengths):\n",
    "        \"\"\"\n",
    "        inputs : B,T # LongTensor\n",
    "        input_lengths : B # list\n",
    "        \"\"\"\n",
    "        \n",
    "        embed = self.embed(inputs)\n",
    "        embed = self.dropout(embed)\n",
    "        \n",
    "        # 패딩된 문장을 패킹(패딩은 연산 안들어가도록)\n",
    "        packed = pack_padded_sequence(embed, input_lengths,batch_first=True) \n",
    "        output, hidden = self.gru(packed)\n",
    "        \n",
    "        # 패킹된 문장을 다시 unpack\n",
    "        output, output_lengths = pad_packed_sequence(output,batch_first=True) \n",
    "        \n",
    "        # last hidden 선택하기 , concat\n",
    "        hidden = hidden[-self.num_directions:] # num_layers*num_directions,batch_size,hidden_size\n",
    "        hidden = torch.cat([h for h in hidden],1).unsqueeze(0) # 1,B,2H\n",
    "        \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,V,E,H,sos_idx,max_len=15):\n",
    "        super(Decoder,self).__init__()\n",
    "        \n",
    "        self.hidden_size = H\n",
    "        self.max_len = max_len\n",
    "        self.sos_idx = sos_idx\n",
    "        self.embed = nn.Embedding(V,E)\n",
    "        self.gru = nn.GRU(E+H,H,batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.linear = nn.Linear(2*H,V)\n",
    "        self.attention = Attention(H,'general') # 어텐션\n",
    "        \n",
    "    def start_token(self,batch_size):\n",
    "        sos = Variable(torch.LongTensor([self.sos_idx]*batch_size)).unsqueeze(1)\n",
    "        if USE_CUDA: sos = sos.cuda()\n",
    "        return sos\n",
    "       \n",
    "    def forward(self,hidden, encoder_hiddens, encoder_lengths=None, max_len=None):\n",
    "        \"\"\"\n",
    "        hidden : 1,B,H (인코더 라스트 히든)\n",
    "        encoder_hiddens : B,T,H (인코더 모든 타임스텝에서 히든스테이트들)\n",
    "        encoder_lengths : B (인풋의 진짜 길이 list)\n",
    "        \"\"\"\n",
    "        if max_len is None: max_len = self.max_len\n",
    "        \n",
    "        inputs = self.start_token(hidden.size(1)) # Batch_size\n",
    "        embed = self.embed(inputs)\n",
    "        embed= self.dropout(embed)\n",
    "        scores=[]\n",
    "        attn_weights=[]\n",
    "        for _ in range(max_len):\n",
    "            \n",
    "            # context vector 계산\n",
    "            context, attn_weight = self.attention(hidden.transpose(0,1), encoder_hiddens, encoder_lengths,True)\n",
    "            attn_weights.append(attn_weight.squeeze(1))\n",
    "            \n",
    "            # concat해서 rnn에\n",
    "            rnn_input = torch.cat([embed,context],2)\n",
    "            _, hidden = self.gru(rnn_input,hidden)\n",
    "            \n",
    "            # concat해서 linear에\n",
    "            concated = torch.cat([hidden.transpose(0,1),context],2)\n",
    "            score = self.linear(concated.squeeze(1))\n",
    "            scores.append(score)\n",
    "            decoded = score.max(1)[1] # greedy\n",
    "            embed = self.embed(decoded).unsqueeze(1) # y_{t-1}\n",
    "            embed = self.dropout(embed)\n",
    "            \n",
    "        #  column-wise concat, reshape!!\n",
    "        scores = torch.cat(scores,1)\n",
    "        return scores.view(inputs.size(0)*max_len,-1), torch.cat(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HIDDEN = 100\n",
    "EMBED = 50\n",
    "STEP = 200\n",
    "LR = 0.001\n",
    "\n",
    "encoder = Encoder(len(SOURCE.vocab),EMBED,HIDDEN,bidirec=True)\n",
    "decoder = Decoder(len(TARGET.vocab),EMBED,HIDDEN*2,TARGET.vocab.stoi['<s>'])\n",
    "\n",
    "if USE_CUDA:\n",
    "    encoder = encoder.cuda()\n",
    "    decoder = decoder.cuda()\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=TARGET.vocab.stoi['<pad>'])\n",
    "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()),lr=LR)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(gamma=0.1,milestones=[100],optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.36419428885\n",
      "3.25571791083\n",
      "2.57027836516\n",
      "2.08639037982\n",
      "1.69171138667\n",
      "1.39513733797\n",
      "1.17431018874\n",
      "0.968443433754\n",
      "0.791452653706\n",
      "0.665851117112\n",
      "0.551195865031\n",
      "0.488491506316\n",
      "0.461174290162\n",
      "0.44934902899\n",
      "0.445566720329\n",
      "0.420567046851\n",
      "0.412186958827\n",
      "0.390605855267\n",
      "0.403296662029\n",
      "0.369189135497\n"
     ]
    }
   ],
   "source": [
    "encoder.train()\n",
    "decoder.train()\n",
    "for step in range(STEP):\n",
    "    losses=[]\n",
    "    scheduler.step()\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        inputs,lengths = batch.inputs\n",
    "        targets = batch.targets\n",
    "        \n",
    "        encoder.zero_grad()\n",
    "        decoder.zero_grad()\n",
    "        \n",
    "        output, hidden = encoder(inputs,lengths.tolist())\n",
    "        preds, _ = decoder(hidden,output,lengths.tolist(),targets.size(1))\n",
    "        \n",
    "        loss = loss_function(preds,targets.view(-1))\n",
    "        losses.append(loss.data[0])\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if step % 10 == 0:\n",
    "        print(np.mean(losses))\n",
    "        losses=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## TEST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요.\n",
      "good morning .\n",
      "도와줘요!\n",
      "help ! .\n",
      "미안해요.\n",
      "i 'm bored to .\n",
      "네, 감사합니다.\n",
      "yes , thank you .\n"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "while 1:\n",
    "    try:\n",
    "        text = input()\n",
    "        tokenized = [\"<s>\"] + kor_tagger(text) +[\"</s>\"]\n",
    "        input_,length = SOURCE.numericalize(([tokenized],[len(tokenized)]),train=False,device=DEVICE)\n",
    "\n",
    "        o,h = encoder(input_,length.tolist())\n",
    "        preds, _ = decoder(h,o,length.tolist())\n",
    "        reply = [TARGET.vocab.itos[i] for i in preds.max(1)[1].data.tolist() if i not in [0,1,2,3]]\n",
    "\n",
    "        print(\" \".join(reply))\n",
    "    except KeyboardInterrupt as e:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
